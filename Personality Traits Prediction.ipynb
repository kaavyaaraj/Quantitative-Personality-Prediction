{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a confusion matrix and\n",
    "# calculate evaluation metrics using it\n",
    "def summarize_metrics(tp, tn, fp, fn):\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1_score = (2 * recall * precision) / (recall + precision)\n",
    "\n",
    "    print(\"Precison:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    status_data = pandas.read_csv(\"mypersonality_final.csv\")\n",
    "\n",
    "    NEG_INDEX = 2\n",
    "    POS_INDEX = 3\n",
    "    NEU_INDEX = 4\n",
    "    COMP_INDEX = 5\n",
    "\n",
    "    # Annotate the status with sentiment scores\n",
    "    # From nltk.sentiment.vader corpus\n",
    "    if not os.path.isfile(\"mypersonality_cleaned.csv\"):\n",
    "        status_data.insert(NEG_INDEX, \"sentiNEG\", 0)\n",
    "        status_data.insert(POS_INDEX, \"sentiPOS\", 0)\n",
    "        status_data.insert(NEU_INDEX, \"sentiNEU\", 0)\n",
    "        status_data.insert(COMP_INDEX, \"sentiCOMPOUND\", 0)\n",
    "\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        count = 0\n",
    "        for row in status_data.itertuples():\n",
    "            \"\"\"\n",
    "            pos: positive\n",
    "            neg: negative\n",
    "            neu: neutral\n",
    "            compound: aggregated score for the sentence\n",
    "            \"\"\"\n",
    "            ss = sid.polarity_scores(row.STATUS)\n",
    "            status_data.iloc[count, NEG_INDEX] = ss[\"neg\"]\n",
    "            status_data.iloc[count, POS_INDEX] = ss[\"pos\"]\n",
    "            status_data.iloc[count, NEU_INDEX] = ss[\"neu\"]\n",
    "            status_data.iloc[count, COMP_INDEX] = ss[\"compound\"]\n",
    "            count += 1\n",
    "\n",
    "        status_data.to_csv(\"mypersonality_cleaned.csv\")\n",
    "    else:\n",
    "        status_data = pandas.read_csv(\"mypersonality_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "status_data = status_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We drop columns which give us a score for personality type\n",
    "status_data = status_data.drop(['STATUS', '#AUTHID', 'sEXT', 'sNEU', 'sAGR',\n",
    "                                    'sCON', 'sOPN', 'DATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop non-normalized scores of Brokerage and Betweenness\n",
    "status_data = status_data.drop(['BROKERAGE', 'BETWEENNESS', 'NBROKERAGE',\n",
    "                                    'NBETWEENNESS', 'DENSITY', 'TRANSITIVITY', 'NETWORKSIZE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiNEG</th>\n",
       "      <th>sentiPOS</th>\n",
       "      <th>sentiNEU</th>\n",
       "      <th>sentiCOMPOUND</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.6841</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.5095</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>9887</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.4993</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>9888</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9889</th>\n",
       "      <td>9889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>9890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>9891</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.3648</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>9892</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>9893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>9894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>9895</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.8217</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>9896</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.9036</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>9897</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>9898</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.6083</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>9899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>9900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>9901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>9902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>9903</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>9904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>9905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>9906</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>9907</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>9908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>9909</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>9910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>9911</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>9912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>9913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>9914</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>9915</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>9916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9916 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentiNEG  sentiPOS  sentiNEU  sentiCOMPOUND cEXT cNEU cAGR  \\\n",
       "0              0     0.000     0.412     0.588         0.4215    n    y    n   \n",
       "1              1     0.167     0.000     0.833        -0.3412    n    y    n   \n",
       "2              2     0.195     0.278     0.527         0.6280    n    y    n   \n",
       "3              3     0.000     0.259     0.741         0.4215    n    y    n   \n",
       "4              4     0.000     0.592     0.408         0.4404    n    y    n   \n",
       "5              5     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "6              6     0.000     0.515     0.485         0.8916    n    y    n   \n",
       "7              7     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "8              8     0.188     0.053     0.759        -0.6249    n    y    n   \n",
       "9              9     0.000     0.323     0.677         0.7351    n    y    n   \n",
       "10            10     0.242     0.000     0.758        -0.1531    n    y    n   \n",
       "11            11     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "12            12     0.136     0.376     0.488         0.6633    n    y    n   \n",
       "13            13     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "14            14     0.000     0.541     0.459         0.7096    n    y    n   \n",
       "15            15     0.000     0.317     0.683         0.4184    n    y    n   \n",
       "16            16     0.000     0.178     0.822         0.3818    n    y    n   \n",
       "17            17     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "18            18     0.266     0.000     0.734        -0.4404    n    y    n   \n",
       "19            19     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "20            20     0.000     0.179     0.821         0.3400    n    y    n   \n",
       "21            21     0.214     0.000     0.786        -0.6841    n    y    n   \n",
       "22            22     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "23            23     0.000     0.430     0.570         0.7003    n    y    n   \n",
       "24            24     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "25            25     0.310     0.000     0.690        -0.5574    n    y    n   \n",
       "26            26     0.000     1.000     0.000         0.4404    n    y    n   \n",
       "27            27     0.351     0.000     0.649        -0.4019    n    y    n   \n",
       "28            28     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "29            29     0.329     0.227     0.445        -0.5095    n    y    n   \n",
       "...          ...       ...       ...       ...            ...  ...  ...  ...   \n",
       "9887        9887     0.144     0.000     0.856        -0.4993    n    y    n   \n",
       "9888        9888     0.000     0.666     0.334         0.7177    n    y    n   \n",
       "9889        9889     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9890        9890     0.000     0.163     0.837         0.5707    n    y    n   \n",
       "9891        9891     0.262     0.158     0.580        -0.3648    n    y    n   \n",
       "9892        9892     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9893        9893     0.000     0.259     0.741         0.4199    n    y    n   \n",
       "9894        9894     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9895        9895     0.136     0.490     0.374         0.8217    n    y    n   \n",
       "9896        9896     0.000     0.603     0.397         0.9036    n    y    n   \n",
       "9897        9897     0.000     0.225     0.775         0.5399    n    y    n   \n",
       "9898        9898     0.407     0.179     0.413        -0.6083    n    y    n   \n",
       "9899        9899     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9900        9900     0.086     0.000     0.914        -0.0572    n    y    n   \n",
       "9901        9901     0.000     0.394     0.606         0.8316    n    y    n   \n",
       "9902        9902     0.000     0.413     0.587         0.9449    y    y    y   \n",
       "9903        9903     0.000     0.286     0.714         0.4215    y    y    y   \n",
       "9904        9904     0.000     0.437     0.563         0.5962    y    y    y   \n",
       "9905        9905     0.000     0.107     0.893         0.4588    y    y    y   \n",
       "9906        9906     0.196     0.324     0.480         0.4939    n    n    n   \n",
       "9907        9907     0.108     0.081     0.811        -0.1779    n    n    n   \n",
       "9908        9908     0.000     0.000     1.000         0.0000    n    n    n   \n",
       "9909        9909     0.266     0.000     0.734        -0.4404    n    y    n   \n",
       "9910        9910     0.000     0.000     1.000         0.0000    y    n    y   \n",
       "9911        9911     0.000     0.000     1.000         0.0000    n    n    y   \n",
       "9912        9912     0.000     0.000     1.000         0.0000    n    n    y   \n",
       "9913        9913     0.000     0.322     0.678         0.2263    y    y    y   \n",
       "9914        9914     0.219     0.000     0.781        -0.1027    y    y    y   \n",
       "9915        9915     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9916        9916     0.000     0.223     0.777         0.8416    y    y    n   \n",
       "\n",
       "     cCON cOPN  \n",
       "0       n    y  \n",
       "1       n    y  \n",
       "2       n    y  \n",
       "3       n    y  \n",
       "4       n    y  \n",
       "5       n    y  \n",
       "6       n    y  \n",
       "7       n    y  \n",
       "8       n    y  \n",
       "9       n    y  \n",
       "10      n    y  \n",
       "11      n    y  \n",
       "12      n    y  \n",
       "13      n    y  \n",
       "14      n    y  \n",
       "15      n    y  \n",
       "16      n    y  \n",
       "17      n    y  \n",
       "18      n    y  \n",
       "19      n    y  \n",
       "20      n    y  \n",
       "21      n    y  \n",
       "22      n    y  \n",
       "23      n    y  \n",
       "24      n    y  \n",
       "25      n    y  \n",
       "26      n    y  \n",
       "27      n    y  \n",
       "28      n    y  \n",
       "29      n    y  \n",
       "...   ...  ...  \n",
       "9887    y    y  \n",
       "9888    y    y  \n",
       "9889    y    y  \n",
       "9890    y    y  \n",
       "9891    y    y  \n",
       "9892    y    y  \n",
       "9893    y    y  \n",
       "9894    y    y  \n",
       "9895    y    y  \n",
       "9896    y    y  \n",
       "9897    y    y  \n",
       "9898    y    y  \n",
       "9899    y    y  \n",
       "9900    y    y  \n",
       "9901    y    y  \n",
       "9902    y    y  \n",
       "9903    y    y  \n",
       "9904    y    y  \n",
       "9905    y    y  \n",
       "9906    y    y  \n",
       "9907    y    y  \n",
       "9908    y    y  \n",
       "9909    n    y  \n",
       "9910    y    n  \n",
       "9911    n    y  \n",
       "9912    n    y  \n",
       "9913    y    y  \n",
       "9914    y    y  \n",
       "9915    n    y  \n",
       "9916    y    y  \n",
       "\n",
       "[9916 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the name of first row from \"Unknown\" to \"rowID\"\n",
    "new_columns = status_data.columns.values\n",
    "new_columns[0] = \"rowID\"\n",
    "status_data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the columns to be predicted, at the end\n",
    "cols = status_data.columns.tolist()\n",
    "cols = cols[:5] + cols[5:10]\n",
    "status_data = status_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowID</th>\n",
       "      <th>sentiNEG</th>\n",
       "      <th>sentiPOS</th>\n",
       "      <th>sentiNEU</th>\n",
       "      <th>sentiCOMPOUND</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.6841</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.5095</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>9887</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.4993</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>9888</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9889</th>\n",
       "      <td>9889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>9890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>9891</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.3648</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>9892</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>9893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>9894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>9895</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.8217</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>9896</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.9036</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>9897</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>9898</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.6083</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>9899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>9900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>9901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>9902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>9903</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>9904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>9905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>9906</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>9907</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>9908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>9909</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>9910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>9911</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>9912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>9913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>9914</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>9915</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>9916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9916 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rowID  sentiNEG  sentiPOS  sentiNEU  sentiCOMPOUND cEXT cNEU cAGR cCON  \\\n",
       "0         0     0.000     0.412     0.588         0.4215    n    y    n    n   \n",
       "1         1     0.167     0.000     0.833        -0.3412    n    y    n    n   \n",
       "2         2     0.195     0.278     0.527         0.6280    n    y    n    n   \n",
       "3         3     0.000     0.259     0.741         0.4215    n    y    n    n   \n",
       "4         4     0.000     0.592     0.408         0.4404    n    y    n    n   \n",
       "5         5     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "6         6     0.000     0.515     0.485         0.8916    n    y    n    n   \n",
       "7         7     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "8         8     0.188     0.053     0.759        -0.6249    n    y    n    n   \n",
       "9         9     0.000     0.323     0.677         0.7351    n    y    n    n   \n",
       "10       10     0.242     0.000     0.758        -0.1531    n    y    n    n   \n",
       "11       11     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "12       12     0.136     0.376     0.488         0.6633    n    y    n    n   \n",
       "13       13     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "14       14     0.000     0.541     0.459         0.7096    n    y    n    n   \n",
       "15       15     0.000     0.317     0.683         0.4184    n    y    n    n   \n",
       "16       16     0.000     0.178     0.822         0.3818    n    y    n    n   \n",
       "17       17     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "18       18     0.266     0.000     0.734        -0.4404    n    y    n    n   \n",
       "19       19     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "20       20     0.000     0.179     0.821         0.3400    n    y    n    n   \n",
       "21       21     0.214     0.000     0.786        -0.6841    n    y    n    n   \n",
       "22       22     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "23       23     0.000     0.430     0.570         0.7003    n    y    n    n   \n",
       "24       24     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "25       25     0.310     0.000     0.690        -0.5574    n    y    n    n   \n",
       "26       26     0.000     1.000     0.000         0.4404    n    y    n    n   \n",
       "27       27     0.351     0.000     0.649        -0.4019    n    y    n    n   \n",
       "28       28     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "29       29     0.329     0.227     0.445        -0.5095    n    y    n    n   \n",
       "...     ...       ...       ...       ...            ...  ...  ...  ...  ...   \n",
       "9887   9887     0.144     0.000     0.856        -0.4993    n    y    n    y   \n",
       "9888   9888     0.000     0.666     0.334         0.7177    n    y    n    y   \n",
       "9889   9889     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9890   9890     0.000     0.163     0.837         0.5707    n    y    n    y   \n",
       "9891   9891     0.262     0.158     0.580        -0.3648    n    y    n    y   \n",
       "9892   9892     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9893   9893     0.000     0.259     0.741         0.4199    n    y    n    y   \n",
       "9894   9894     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9895   9895     0.136     0.490     0.374         0.8217    n    y    n    y   \n",
       "9896   9896     0.000     0.603     0.397         0.9036    n    y    n    y   \n",
       "9897   9897     0.000     0.225     0.775         0.5399    n    y    n    y   \n",
       "9898   9898     0.407     0.179     0.413        -0.6083    n    y    n    y   \n",
       "9899   9899     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9900   9900     0.086     0.000     0.914        -0.0572    n    y    n    y   \n",
       "9901   9901     0.000     0.394     0.606         0.8316    n    y    n    y   \n",
       "9902   9902     0.000     0.413     0.587         0.9449    y    y    y    y   \n",
       "9903   9903     0.000     0.286     0.714         0.4215    y    y    y    y   \n",
       "9904   9904     0.000     0.437     0.563         0.5962    y    y    y    y   \n",
       "9905   9905     0.000     0.107     0.893         0.4588    y    y    y    y   \n",
       "9906   9906     0.196     0.324     0.480         0.4939    n    n    n    y   \n",
       "9907   9907     0.108     0.081     0.811        -0.1779    n    n    n    y   \n",
       "9908   9908     0.000     0.000     1.000         0.0000    n    n    n    y   \n",
       "9909   9909     0.266     0.000     0.734        -0.4404    n    y    n    n   \n",
       "9910   9910     0.000     0.000     1.000         0.0000    y    n    y    y   \n",
       "9911   9911     0.000     0.000     1.000         0.0000    n    n    y    n   \n",
       "9912   9912     0.000     0.000     1.000         0.0000    n    n    y    n   \n",
       "9913   9913     0.000     0.322     0.678         0.2263    y    y    y    y   \n",
       "9914   9914     0.219     0.000     0.781        -0.1027    y    y    y    y   \n",
       "9915   9915     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "9916   9916     0.000     0.223     0.777         0.8416    y    y    n    y   \n",
       "\n",
       "     cOPN  \n",
       "0       y  \n",
       "1       y  \n",
       "2       y  \n",
       "3       y  \n",
       "4       y  \n",
       "5       y  \n",
       "6       y  \n",
       "7       y  \n",
       "8       y  \n",
       "9       y  \n",
       "10      y  \n",
       "11      y  \n",
       "12      y  \n",
       "13      y  \n",
       "14      y  \n",
       "15      y  \n",
       "16      y  \n",
       "17      y  \n",
       "18      y  \n",
       "19      y  \n",
       "20      y  \n",
       "21      y  \n",
       "22      y  \n",
       "23      y  \n",
       "24      y  \n",
       "25      y  \n",
       "26      y  \n",
       "27      y  \n",
       "28      y  \n",
       "29      y  \n",
       "...   ...  \n",
       "9887    y  \n",
       "9888    y  \n",
       "9889    y  \n",
       "9890    y  \n",
       "9891    y  \n",
       "9892    y  \n",
       "9893    y  \n",
       "9894    y  \n",
       "9895    y  \n",
       "9896    y  \n",
       "9897    y  \n",
       "9898    y  \n",
       "9899    y  \n",
       "9900    y  \n",
       "9901    y  \n",
       "9902    y  \n",
       "9903    y  \n",
       "9904    y  \n",
       "9905    y  \n",
       "9906    y  \n",
       "9907    y  \n",
       "9908    y  \n",
       "9909    y  \n",
       "9910    n  \n",
       "9911    y  \n",
       "9912    y  \n",
       "9913    y  \n",
       "9914    y  \n",
       "9915    y  \n",
       "9916    y  \n",
       "\n",
       "[9916 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'y' for 1 and 'n' for 0\n",
    "features = ['cEXT', 'cNEU', 'cOPN', 'cAGR', 'cCON']\n",
    "for feature in features:\n",
    "    status_data[feature] = status_data[feature].map({'y': 1.0, 'n': 0.0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and test data: 66% and 33%\n",
    "train_data, test_data = train_test_split(status_data, test_size=0.50)\n",
    "\n",
    "train = train_data.values\n",
    "test = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a classifier\n",
    "# k is chosen to be square root of number of training example\n",
    "model = KNeighborsClassifier(n_neighbors=250)\n",
    "model = model.fit(train[0:, 1:5], train[0:, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.46200000e+03,   3.89000000e-01,   1.90000000e-01,\n",
       "         4.21000000e-01,  -2.24400000e-01,   0.00000000e+00,\n",
       "         1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.00000000e+00])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "output = model.predict(test[:, 1:5])\n",
    "rowID = [TEST.rowID for TEST in test_data.itertuples()]\n",
    "result_df = pandas.DataFrame({\"rowID\": rowID,\"cOPN\": list(output)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265 121 1148 424\n",
      "Precison: 0.7398595060049853\n",
      "Recall: 0.8850637029005151\n",
      "Accuracy: 0.6829366680112948\n",
      "F1 score: 0.8059738336213281\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for row in rowID:\n",
    "    test_cEXT_val = int(test_data.loc[test_data['rowID'] == row].cOPN)\n",
    "    result_cEXT_val = int(result_df.loc[result_df['rowID'] == row].cOPN)\n",
    "    if test_cEXT_val == 1:\n",
    "        if result_cEXT_val == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_SVM = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_SVM = model_SVM.fit(train[0:, 1:5], train[0:, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict_SVM\n",
    "output_SVM = model_SVM.predict(test[:, 1:5])\n",
    "rowID_SVM = [TEST.rowID for TEST in test_data.itertuples()]\n",
    "result_df_SVM = pandas.DataFrame({\"rowID\": rowID,\"cOPN\": list(output_SVM)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3545 33 1236 144\n",
      "Precison: 0.7414766785191382\n",
      "Recall: 0.9609650311737599\n",
      "Accuracy: 0.7216619604679306\n",
      "F1 score: 0.8370720188902008\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for row in rowID:\n",
    "    test_cEXT_val = int(test_data.loc[test_data['rowID'] == row].cOPN)\n",
    "    result_cEXT_val_SVM = int(result_df_SVM.loc[result_df['rowID'] == row].cOPN)\n",
    "    if test_cEXT_val == 1:\n",
    "        if result_cEXT_val_SVM == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_SVM == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gnb = model_gnb.fit(train[0:, 1:5], train[0:, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.46200000e+03,   3.89000000e-01,   1.90000000e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  4.54600000e+03,   2.71000000e-01,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  5.78300000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  9.20000000e+03,   3.58000000e-01,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "       [  9.84000000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "       [  8.50900000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict_gnb\n",
    "output_gnb = model_gnb.predict(test[:, 1:5])\n",
    "rowID_gnb = [TEST.rowID for TEST in test_data.itertuples()]\n",
    "result_df_gnb = pandas.DataFrame({\"rowID\": rowID,\"cOPN\": list(output_gnb)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935 240 1029 754\n",
      "Precison: 0.7404137235116044\n",
      "Recall: 0.795608566007048\n",
      "Accuracy: 0.6403791851553046\n",
      "F1 score: 0.7670194694890893\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for row in rowID:\n",
    "    test_cEXT_val = int(test_data.loc[test_data['rowID'] == row].cOPN)\n",
    "    result_cEXT_val_gnb = int(result_df_gnb.loc[result_df['rowID'] == row].cOPN)\n",
    "    if test_cEXT_val == 1:\n",
    "        if result_cEXT_val_gnb == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_gnb == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Second Approach, using Doc2Vec and LSTM for predicting\n",
    "import gensim\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "docLabels_y = []\n",
    "docLabels_y = [f for f in listdir(\"./Dataset Processed/txt output_final/y\") if f.endswith('.txt')]\n",
    "docLabels_n = []\n",
    "docLabels_n = [f for f in listdir(\"./Dataset Processed/txt output_final/n\") if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "data_y = []\n",
    "for doc in docLabels_y:\n",
    "    f = open(\"./Dataset Processed/txt output_final/y/\" + doc, 'r')\n",
    "    data_y.append(f)\n",
    "    data.append(f)\n",
    "    f.close()\n",
    "data_n = []\n",
    "for doc in docLabels_n:\n",
    "    f1 = open(\"./Dataset Processed/txt output_final/n/\" + doc, 'r')\n",
    "    data_n.append(f1)\n",
    "    data.append(f1)\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LabeledSentence = gensim.models.doc2vec.LabeledSentence  \n",
    "  \n",
    "# class LabeledLineSentence(object):  \n",
    "#     def __init__(self, sentences):  \n",
    "#         self.sentences = sentences  \n",
    "#     def __iter__(self):  \n",
    "#         for id, line in enumerate(sentences):  \n",
    "#             yield LabeledSentence(words=line, tags=['SENT_%s' % id])  \n",
    "#     def sentences_perm(self):\n",
    "#         shuffle(self.sentences)\n",
    "#         return self.sentences\n",
    "              \n",
    "# it = LabeledLineSentence(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 00:00:04,754 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c25d30b7e267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0municode\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, progress_per, trim_rule, update)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6043c56a5bd9>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SENT_%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences_perm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(size=100, window=10, min_count=1, workers=11, alpha=0.025, min_alpha=0.025)  \n",
    "model.build_vocab(it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 23:58:19,747 : INFO : running C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py -f C:\\Users\\tianx\\AppData\\Roaming\\jupyter\\runtime\\kernel-1525645a-5d42-4290-ad5f-0978609f7a3f.json\n",
      "2018-04-24 23:58:19,752 : INFO : Epoch 0\n",
      "2018-04-24 23:58:19,770 : INFO : training model with 11 workers on 0 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-cf447f146636>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     model.train(it.sentences_perm(),\n\u001b[0;32m     14\u001b[0m                 \u001b[1;31m#total_examples=model.corpus_count,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first finalize vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from random import shuffle\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "for epoch in range(100):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(it.sentences_perm(),\n",
    "                #total_examples=model.corpus_count,\n",
    "                epochs=model.iter,\n",
    "    )\n",
    "\n",
    "model.save('./Doc2Vec_model.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:42:12,740 : INFO : running C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py -f C:\\Users\\tianx\\AppData\\Roaming\\jupyter\\runtime\\kernel-2864b082-d451-44be-9948-6e712fdda201.json\n",
      "2018-04-25 14:42:12,893 : INFO : collecting all words and their counts\n",
      "2018-04-25 14:42:12,897 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-04-25 14:42:12,973 : INFO : collected 15770 word types and 5364 unique tags from a corpus of 5364 examples and 72630 words\n",
      "2018-04-25 14:42:12,981 : INFO : Loading a fresh vocabulary\n",
      "2018-04-25 14:42:13,096 : INFO : min_count=1 retains 15770 unique words (100% of original 15770, drops 0)\n",
      "2018-04-25 14:42:13,103 : INFO : min_count=1 leaves 72630 word corpus (100% of original 72630, drops 0)\n",
      "2018-04-25 14:42:13,282 : INFO : deleting the raw counts dictionary of 15770 items\n",
      "2018-04-25 14:42:13,287 : INFO : sample=0.0001 downsamples 406 most-common words\n",
      "2018-04-25 14:42:13,291 : INFO : downsampling leaves estimated 41246 word corpus (56.8% of prior 72630)\n",
      "2018-04-25 14:42:13,295 : INFO : estimated required memory for 15770 words and 100 dimensions: 23719400 bytes\n",
      "2018-04-25 14:42:13,503 : INFO : resetting layer weights\n",
      "2018-04-25 14:42:14,127 : INFO : Epoch 0\n",
      "2018-04-25 14:42:14,143 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:15,358 : INFO : PROGRESS: at 21.91% examples, 44599 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:16,369 : INFO : PROGRESS: at 60.23% examples, 65143 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:16,901 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:16,911 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:16,919 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:16,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:16,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:16,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:17,012 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:17,018 : INFO : training on 363150 raw words (233054 effective words) took 2.8s, 82933 effective words/s\n",
      "2018-04-25 14:42:17,022 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:17,025 : INFO : Epoch 1\n",
      "2018-04-25 14:42:17,041 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:18,055 : INFO : PROGRESS: at 21.94% examples, 51267 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:19,077 : INFO : PROGRESS: at 63.12% examples, 72841 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:19,596 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:19,604 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:19,609 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:19,622 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:19,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:19,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:19,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:19,691 : INFO : training on 363150 raw words (233292 effective words) took 2.6s, 88525 effective words/s\n",
      "2018-04-25 14:42:19,696 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:19,699 : INFO : Epoch 2\n",
      "2018-04-25 14:42:19,710 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:20,725 : INFO : PROGRESS: at 22.02% examples, 51536 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:21,761 : INFO : PROGRESS: at 63.34% examples, 72501 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:22,238 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:22,250 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:22,257 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:22,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:22,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:22,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:22,335 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:22,342 : INFO : training on 363150 raw words (233342 effective words) took 2.6s, 89101 effective words/s\n",
      "2018-04-25 14:42:22,345 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:22,348 : INFO : Epoch 3\n",
      "2018-04-25 14:42:22,360 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:23,687 : INFO : PROGRESS: at 41.28% examples, 73559 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:24,667 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:24,724 : INFO : PROGRESS: at 87.99% examples, 87481 words/s, in_qsize 5, out_qsize 1\n",
      "2018-04-25 14:42:24,734 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:24,744 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:24,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:24,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:24,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:24,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:24,808 : INFO : training on 363150 raw words (232834 effective words) took 2.4s, 95920 effective words/s\n",
      "2018-04-25 14:42:24,811 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:24,813 : INFO : Epoch 4\n",
      "2018-04-25 14:42:24,826 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:25,861 : INFO : PROGRESS: at 38.56% examples, 88087 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:27,115 : INFO : PROGRESS: at 79.78% examples, 81685 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:42:27,163 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:27,170 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:27,175 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:27,181 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:27,207 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:27,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:27,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:27,283 : INFO : training on 363150 raw words (232845 effective words) took 2.4s, 95286 effective words/s\n",
      "2018-04-25 14:42:27,287 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:27,290 : INFO : Epoch 5\n",
      "2018-04-25 14:42:27,307 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:28,336 : INFO : PROGRESS: at 35.79% examples, 82673 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:29,568 : INFO : PROGRESS: at 79.81% examples, 83042 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:42:29,633 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:29,649 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:29,656 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:29,663 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:29,695 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:29,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:29,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:29,740 : INFO : training on 363150 raw words (232957 effective words) took 2.4s, 96655 effective words/s\n",
      "2018-04-25 14:42:29,743 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:29,746 : INFO : Epoch 6\n",
      "2018-04-25 14:42:29,759 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:30,826 : INFO : PROGRESS: at 21.96% examples, 48439 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:31,875 : INFO : PROGRESS: at 60.48% examples, 66931 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:32,398 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:32,402 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:32,411 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:32,421 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:32,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:32,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:32,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:32,481 : INFO : training on 363150 raw words (232939 effective words) took 2.7s, 85921 effective words/s\n",
      "2018-04-25 14:42:32,483 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:32,486 : INFO : Epoch 7\n",
      "2018-04-25 14:42:32,500 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:33,942 : INFO : PROGRESS: at 41.11% examples, 67601 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:34,809 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:34,817 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:34,859 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:34,869 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:34,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:34,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:34,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:34,944 : INFO : training on 363150 raw words (233188 effective words) took 2.4s, 96060 effective words/s\n",
      "2018-04-25 14:42:34,947 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:34,951 : INFO : Epoch 8\n",
      "2018-04-25 14:42:34,961 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:36,316 : INFO : PROGRESS: at 41.25% examples, 72011 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:37,167 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:37,171 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:37,180 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:37,188 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:37,195 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:37,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:37,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:37,265 : INFO : training on 363150 raw words (233182 effective words) took 2.3s, 102058 effective words/s\n",
      "2018-04-25 14:42:37,268 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:37,270 : INFO : Epoch 9\n",
      "2018-04-25 14:42:37,281 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:38,587 : INFO : PROGRESS: at 41.20% examples, 74358 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:39,484 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:39,509 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:39,517 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:39,529 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:39,537 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:39,550 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:39,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:39,592 : INFO : training on 363150 raw words (232904 effective words) took 2.3s, 101469 effective words/s\n",
      "2018-04-25 14:42:39,596 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:39,601 : INFO : Epoch 10\n",
      "2018-04-25 14:42:39,614 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:40,930 : INFO : PROGRESS: at 41.17% examples, 74239 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:41,825 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:41,838 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:41,856 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:41,887 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:41,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:41,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:41,956 : INFO : PROGRESS: at 100.00% examples, 100393 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:41,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:41,967 : INFO : training on 363150 raw words (233178 effective words) took 2.3s, 99915 effective words/s\n",
      "2018-04-25 14:42:41,972 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:41,975 : INFO : Epoch 11\n",
      "2018-04-25 14:42:41,991 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:43,293 : INFO : PROGRESS: at 41.16% examples, 74791 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:44,234 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:44,240 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:44,248 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:44,258 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:44,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:44,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:44,323 : INFO : PROGRESS: at 100.00% examples, 100666 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:44,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:44,331 : INFO : training on 363150 raw words (232961 effective words) took 2.3s, 100329 effective words/s\n",
      "2018-04-25 14:42:44,334 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:44,337 : INFO : Epoch 12\n",
      "2018-04-25 14:42:44,348 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:45,705 : INFO : PROGRESS: at 41.25% examples, 71582 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:46,634 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:46,643 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:46,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:46,655 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:46,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:46,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:46,718 : INFO : PROGRESS: at 100.00% examples, 98949 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:46,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:46,725 : INFO : training on 363150 raw words (233069 effective words) took 2.4s, 98676 effective words/s\n",
      "2018-04-25 14:42:46,727 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:46,730 : INFO : Epoch 13\n",
      "2018-04-25 14:42:46,741 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:48,062 : INFO : PROGRESS: at 41.23% examples, 74088 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:48,946 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:48,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:48,960 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:48,969 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:48,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:49,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:49,068 : INFO : PROGRESS: at 100.00% examples, 101127 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:49,074 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:49,079 : INFO : training on 363150 raw words (233434 effective words) took 2.3s, 100668 effective words/s\n",
      "2018-04-25 14:42:49,083 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:49,086 : INFO : Epoch 14\n",
      "2018-04-25 14:42:49,103 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:50,491 : INFO : PROGRESS: at 41.14% examples, 69901 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:51,359 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:51,365 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:51,373 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:51,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:51,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:51,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:51,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:51,466 : INFO : training on 363150 raw words (232873 effective words) took 2.3s, 99111 effective words/s\n",
      "2018-04-25 14:42:51,469 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:51,472 : INFO : Epoch 15\n",
      "2018-04-25 14:42:51,485 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:52,561 : INFO : PROGRESS: at 21.88% examples, 48428 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:53,693 : INFO : PROGRESS: at 60.56% examples, 64386 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:54,252 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:54,278 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:54,284 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:54,314 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:54,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:54,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:54,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:54,384 : INFO : training on 363150 raw words (232793 effective words) took 2.9s, 80851 effective words/s\n",
      "2018-04-25 14:42:54,387 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:54,390 : INFO : Epoch 16\n",
      "2018-04-25 14:42:54,404 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:55,456 : INFO : PROGRESS: at 24.70% examples, 55526 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:56,568 : INFO : PROGRESS: at 60.51% examples, 65606 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:57,139 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:57,188 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:57,206 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:57,246 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:57,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:57,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:57,333 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:57,344 : INFO : training on 363150 raw words (232912 effective words) took 2.9s, 79607 effective words/s\n",
      "2018-04-25 14:42:57,348 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:57,354 : INFO : Epoch 17\n",
      "2018-04-25 14:42:57,381 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:58,685 : INFO : PROGRESS: at 22.04% examples, 41053 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:00,224 : INFO : PROGRESS: at 60.50% examples, 50557 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:00,896 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:00,962 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:00,987 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:01,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:01,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:01,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:01,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:01,135 : INFO : training on 363150 raw words (233067 effective words) took 3.7s, 62956 effective words/s\n",
      "2018-04-25 14:43:01,139 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:01,142 : INFO : Epoch 18\n",
      "2018-04-25 14:43:01,161 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:02,435 : INFO : PROGRESS: at 21.89% examples, 41711 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:03,677 : INFO : PROGRESS: at 60.47% examples, 57052 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:04,357 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:04,387 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:04,437 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:04,444 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:04,448 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:04,451 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:04,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:04,479 : INFO : training on 363150 raw words (232988 effective words) took 3.3s, 71232 effective words/s\n",
      "2018-04-25 14:43:04,484 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:04,487 : INFO : Epoch 19\n",
      "2018-04-25 14:43:04,498 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:05,622 : INFO : PROGRESS: at 21.99% examples, 46127 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:06,936 : INFO : PROGRESS: at 79.77% examples, 76806 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:43:06,978 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:06,985 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:06,999 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:07,018 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:07,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:07,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:07,086 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:07,090 : INFO : training on 363150 raw words (233387 effective words) took 2.6s, 90498 effective words/s\n",
      "2018-04-25 14:43:07,093 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:07,095 : INFO : Epoch 20\n",
      "2018-04-25 14:43:07,108 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:08,166 : INFO : PROGRESS: at 30.29% examples, 68258 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:09,314 : INFO : PROGRESS: at 60.41% examples, 64641 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:09,829 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:09,841 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:09,848 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:09,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:09,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:09,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:09,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:09,944 : INFO : training on 363150 raw words (232984 effective words) took 2.8s, 82914 effective words/s\n",
      "2018-04-25 14:43:09,947 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:09,950 : INFO : Epoch 21\n",
      "2018-04-25 14:43:09,966 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:11,466 : INFO : PROGRESS: at 22.04% examples, 34557 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:12,595 : INFO : PROGRESS: at 60.37% examples, 53941 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:13,241 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:13,249 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:13,269 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:13,292 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:13,303 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:13,334 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:13,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:13,394 : INFO : training on 363150 raw words (233291 effective words) took 3.4s, 68358 effective words/s\n",
      "2018-04-25 14:43:13,397 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:13,405 : INFO : Epoch 22\n",
      "2018-04-25 14:43:13,428 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:14,626 : INFO : PROGRESS: at 22.02% examples, 43789 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:15,659 : INFO : PROGRESS: at 57.69% examples, 61060 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:16,569 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:16,592 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:16,611 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:16,621 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:16,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:16,666 : INFO : PROGRESS: at 97.10% examples, 70551 words/s, in_qsize 1, out_qsize 1\n",
      "2018-04-25 14:43:16,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:16,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:16,732 : INFO : training on 363150 raw words (233137 effective words) took 3.3s, 71121 effective words/s\n",
      "2018-04-25 14:43:16,735 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:16,745 : INFO : Epoch 23\n",
      "2018-04-25 14:43:16,768 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:18,216 : INFO : PROGRESS: at 21.83% examples, 46117 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:19,217 : INFO : PROGRESS: at 71.45% examples, 79000 words/s, in_qsize 11, out_qsize 0\n",
      "2018-04-25 14:43:19,667 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:19,673 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:19,720 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:19,729 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:19,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:19,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:19,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:19,772 : INFO : training on 363150 raw words (232937 effective words) took 2.7s, 87417 effective words/s\n",
      "2018-04-25 14:43:19,775 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:19,779 : INFO : Epoch 24\n",
      "2018-04-25 14:43:19,790 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:20,953 : INFO : PROGRESS: at 21.98% examples, 44921 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:22,001 : INFO : PROGRESS: at 60.47% examples, 64384 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:22,519 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:22,537 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:22,547 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:22,578 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:22,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:22,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:22,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:22,644 : INFO : training on 363150 raw words (233128 effective words) took 2.8s, 82327 effective words/s\n",
      "2018-04-25 14:43:22,646 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:22,649 : INFO : Epoch 25\n",
      "2018-04-25 14:43:22,661 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:23,689 : INFO : PROGRESS: at 24.62% examples, 56886 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:24,694 : INFO : PROGRESS: at 68.54% examples, 79238 words/s, in_qsize 12, out_qsize 0\n",
      "2018-04-25 14:43:25,161 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:25,166 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:25,223 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:25,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:25,248 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:25,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:25,302 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:25,309 : INFO : training on 363150 raw words (232780 effective words) took 2.6s, 88316 effective words/s\n",
      "2018-04-25 14:43:25,312 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:25,316 : INFO : Epoch 26\n",
      "2018-04-25 14:43:25,332 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:26,361 : INFO : PROGRESS: at 27.33% examples, 63168 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:27,363 : INFO : PROGRESS: at 71.34% examples, 82606 words/s, in_qsize 11, out_qsize 0\n",
      "2018-04-25 14:43:27,854 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:27,869 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:27,880 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:27,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:27,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:27,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:27,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:28,006 : INFO : training on 363150 raw words (233019 effective words) took 2.7s, 87775 effective words/s\n",
      "2018-04-25 14:43:28,009 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:28,013 : INFO : Epoch 27\n",
      "2018-04-25 14:43:28,031 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:29,128 : INFO : PROGRESS: at 21.93% examples, 48367 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:30,132 : INFO : PROGRESS: at 68.71% examples, 77726 words/s, in_qsize 12, out_qsize 0\n",
      "2018-04-25 14:43:30,643 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:30,661 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:30,667 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:30,674 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:30,679 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:30,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:30,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:30,768 : INFO : training on 363150 raw words (233155 effective words) took 2.7s, 86335 effective words/s\n",
      "2018-04-25 14:43:30,772 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:30,775 : INFO : Epoch 28\n",
      "2018-04-25 14:43:30,792 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:31,857 : INFO : PROGRESS: at 21.90% examples, 49076 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:32,858 : INFO : PROGRESS: at 76.91% examples, 87791 words/s, in_qsize 9, out_qsize 0\n",
      "2018-04-25 14:43:33,317 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:33,333 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:33,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:33,352 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:33,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:33,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:33,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:33,435 : INFO : training on 363150 raw words (233265 effective words) took 2.6s, 88920 effective words/s\n",
      "2018-04-25 14:43:33,438 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:33,441 : INFO : Epoch 29\n",
      "2018-04-25 14:43:33,456 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:34,528 : INFO : PROGRESS: at 21.76% examples, 48655 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:35,535 : INFO : PROGRESS: at 68.59% examples, 77759 words/s, in_qsize 12, out_qsize 0\n",
      "2018-04-25 14:43:36,093 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:36,102 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:36,107 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:36,127 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:36,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:36,175 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:36,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:36,214 : INFO : training on 363150 raw words (233115 effective words) took 2.7s, 85140 effective words/s\n",
      "2018-04-25 14:43:36,216 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:36,219 : INFO : Epoch 30\n",
      "2018-04-25 14:43:36,240 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:37,258 : INFO : PROGRESS: at 38.56% examples, 89692 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:38,470 : INFO : PROGRESS: at 79.77% examples, 83988 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:43:38,491 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:38,545 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:38,561 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:38,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:38,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:38,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:38,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:38,648 : INFO : training on 363150 raw words (233046 effective words) took 2.4s, 97430 effective words/s\n",
      "2018-04-25 14:43:38,651 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:38,655 : INFO : Epoch 31\n",
      "2018-04-25 14:43:38,667 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:39,799 : INFO : PROGRESS: at 24.65% examples, 51737 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:40,844 : INFO : PROGRESS: at 74.07% examples, 80050 words/s, in_qsize 10, out_qsize 0\n",
      "2018-04-25 14:43:41,191 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:41,206 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:41,235 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:41,275 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:41,286 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:41,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:41,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:41,331 : INFO : training on 363150 raw words (233075 effective words) took 2.6s, 87962 effective words/s\n",
      "2018-04-25 14:43:41,334 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:41,337 : INFO : Epoch 32\n",
      "2018-04-25 14:43:41,352 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:42,737 : INFO : PROGRESS: at 41.23% examples, 70237 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:43,617 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:43,636 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:43,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:43,657 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:43,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:43,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:43,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:43,724 : INFO : training on 363150 raw words (233134 effective words) took 2.4s, 98895 effective words/s\n",
      "2018-04-25 14:43:43,728 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:43,733 : INFO : Epoch 33\n",
      "2018-04-25 14:43:43,747 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:45,103 : INFO : PROGRESS: at 41.15% examples, 71817 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:45,968 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:45,984 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:45,990 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:46,004 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:46,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:46,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:46,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:46,084 : INFO : training on 363150 raw words (233019 effective words) took 2.3s, 100585 effective words/s\n",
      "2018-04-25 14:43:46,088 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:46,092 : INFO : Epoch 34\n",
      "2018-04-25 14:43:46,103 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:47,128 : INFO : PROGRESS: at 32.73% examples, 76361 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:48,356 : INFO : PROGRESS: at 79.61% examples, 83148 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:43:48,377 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:48,386 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:48,393 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:48,395 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:48,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:48,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:48,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:48,486 : INFO : training on 363150 raw words (232694 effective words) took 2.4s, 98466 effective words/s\n",
      "2018-04-25 14:43:48,489 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:48,492 : INFO : Epoch 35\n",
      "2018-04-25 14:43:48,506 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:49,877 : INFO : PROGRESS: at 41.24% examples, 70921 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:50,762 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:50,777 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:50,799 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:50,804 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:50,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:50,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:50,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:50,874 : INFO : training on 363150 raw words (232997 effective words) took 2.3s, 99164 effective words/s\n",
      "2018-04-25 14:43:50,877 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:50,881 : INFO : Epoch 36\n",
      "2018-04-25 14:43:50,894 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:52,269 : INFO : PROGRESS: at 41.07% examples, 70355 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:53,127 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:53,154 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:53,189 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:53,199 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:53,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:53,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:53,271 : INFO : PROGRESS: at 100.00% examples, 98557 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:43:53,280 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:53,284 : INFO : training on 363150 raw words (233034 effective words) took 2.4s, 98012 effective words/s\n",
      "2018-04-25 14:43:53,289 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:53,292 : INFO : Epoch 37\n",
      "2018-04-25 14:43:53,304 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:54,655 : INFO : PROGRESS: at 41.23% examples, 71829 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:55,498 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:55,503 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:55,512 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:55,520 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:55,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:55,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:55,589 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:55,594 : INFO : training on 363150 raw words (233000 effective words) took 2.3s, 102330 effective words/s\n",
      "2018-04-25 14:43:55,598 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:55,600 : INFO : Epoch 38\n",
      "2018-04-25 14:43:55,611 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:56,978 : INFO : PROGRESS: at 41.23% examples, 70880 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:57,831 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:57,835 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:57,848 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:57,855 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:57,866 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:57,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:57,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:57,961 : INFO : training on 363150 raw words (232842 effective words) took 2.3s, 99591 effective words/s\n",
      "2018-04-25 14:43:57,964 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:57,969 : INFO : Epoch 39\n",
      "2018-04-25 14:43:57,985 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:59,317 : INFO : PROGRESS: at 41.24% examples, 73229 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:00,196 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:00,226 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:00,237 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:00,252 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:00,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:00,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:00,334 : INFO : PROGRESS: at 100.00% examples, 100022 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:44:00,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:00,339 : INFO : training on 363150 raw words (233084 effective words) took 2.3s, 99780 effective words/s\n",
      "2018-04-25 14:44:00,342 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:00,345 : INFO : Epoch 40\n",
      "2018-04-25 14:44:00,356 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:01,699 : INFO : PROGRESS: at 41.22% examples, 72499 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:02,591 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:02,600 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:02,616 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:02,624 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:02,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:02,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:02,708 : INFO : PROGRESS: at 100.00% examples, 99894 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:44:02,711 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:02,714 : INFO : training on 363150 raw words (233126 effective words) took 2.3s, 99648 effective words/s\n",
      "2018-04-25 14:44:02,719 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:02,722 : INFO : Epoch 41\n",
      "2018-04-25 14:44:02,738 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:04,052 : INFO : PROGRESS: at 41.19% examples, 74044 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:04,957 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:04,962 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:04,968 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:04,973 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:04,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:05,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:05,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:05,042 : INFO : training on 363150 raw words (232945 effective words) took 2.3s, 101695 effective words/s\n",
      "2018-04-25 14:44:05,045 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:05,048 : INFO : Epoch 42\n",
      "2018-04-25 14:44:05,061 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:06,399 : INFO : PROGRESS: at 41.21% examples, 72454 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:07,247 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:07,252 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:07,272 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:07,282 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:07,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:07,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:07,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:07,355 : INFO : training on 363150 raw words (232956 effective words) took 2.3s, 102156 effective words/s\n",
      "2018-04-25 14:44:07,357 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:07,361 : INFO : Epoch 43\n",
      "2018-04-25 14:44:07,373 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:08,717 : INFO : PROGRESS: at 41.29% examples, 72178 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:09,578 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:09,584 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:09,616 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:09,623 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:09,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:09,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:09,684 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:09,689 : INFO : training on 363150 raw words (232823 effective words) took 2.3s, 101040 effective words/s\n",
      "2018-04-25 14:44:09,692 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:09,695 : INFO : Epoch 44\n",
      "2018-04-25 14:44:09,706 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:11,070 : INFO : PROGRESS: at 41.16% examples, 71559 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:11,901 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:11,905 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:11,921 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:11,927 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:11,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:11,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:12,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:12,039 : INFO : training on 363150 raw words (233285 effective words) took 2.3s, 100859 effective words/s\n",
      "2018-04-25 14:44:12,043 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:12,047 : INFO : Epoch 45\n",
      "2018-04-25 14:44:12,060 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:13,420 : INFO : PROGRESS: at 41.34% examples, 71670 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:14,289 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:14,299 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:14,307 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:14,316 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:14,324 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:14,351 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:14,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:14,386 : INFO : training on 363150 raw words (232988 effective words) took 2.3s, 100851 effective words/s\n",
      "2018-04-25 14:44:14,389 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:14,391 : INFO : Epoch 46\n",
      "2018-04-25 14:44:14,405 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:15,755 : INFO : PROGRESS: at 41.21% examples, 71858 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:16,604 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:16,610 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:16,621 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:16,650 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:16,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:16,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:16,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:16,718 : INFO : training on 363150 raw words (232966 effective words) took 2.3s, 101265 effective words/s\n",
      "2018-04-25 14:44:16,721 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:16,724 : INFO : Epoch 47\n",
      "2018-04-25 14:44:16,736 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:18,045 : INFO : PROGRESS: at 41.01% examples, 74172 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:18,935 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:18,947 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:18,961 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:18,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:19,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:19,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:19,047 : INFO : PROGRESS: at 100.00% examples, 101543 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:44:19,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:19,055 : INFO : training on 363150 raw words (233124 effective words) took 2.3s, 101177 effective words/s\n",
      "2018-04-25 14:44:19,057 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:19,059 : INFO : Epoch 48\n",
      "2018-04-25 14:44:19,071 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:20,421 : INFO : PROGRESS: at 41.11% examples, 72399 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:21,279 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:21,287 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:21,300 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:21,309 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:21,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:21,342 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:21,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:21,377 : INFO : training on 363150 raw words (233129 effective words) took 2.3s, 102046 effective words/s\n",
      "2018-04-25 14:44:21,379 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:21,382 : INFO : Epoch 49\n",
      "2018-04-25 14:44:21,397 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:22,770 : INFO : PROGRESS: at 41.28% examples, 70988 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:23,623 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:23,636 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:23,653 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:23,676 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:23,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:23,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:23,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:23,733 : INFO : training on 363150 raw words (233056 effective words) took 2.3s, 100575 effective words/s\n",
      "2018-04-25 14:44:23,736 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:23,739 : INFO : Epoch 50\n",
      "2018-04-25 14:44:23,754 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:25,078 : INFO : PROGRESS: at 41.23% examples, 73229 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:25,981 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:25,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:25,999 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:26,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:26,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:26,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:26,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:26,072 : INFO : training on 363150 raw words (232849 effective words) took 2.3s, 100999 effective words/s\n",
      "2018-04-25 14:44:26,076 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:26,079 : INFO : Epoch 51\n",
      "2018-04-25 14:44:26,093 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:27,409 : INFO : PROGRESS: at 41.19% examples, 74886 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:28,284 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:28,312 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:28,323 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:28,332 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:28,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:28,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:28,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:28,397 : INFO : training on 363150 raw words (233162 effective words) took 2.3s, 102580 effective words/s\n",
      "2018-04-25 14:44:28,400 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:28,404 : INFO : Epoch 52\n",
      "2018-04-25 14:44:28,416 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:29,727 : INFO : PROGRESS: at 41.26% examples, 74129 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:30,601 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:30,606 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:30,614 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:30,645 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:30,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:30,666 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:30,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:30,709 : INFO : training on 363150 raw words (233136 effective words) took 2.3s, 102308 effective words/s\n",
      "2018-04-25 14:44:30,712 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:30,715 : INFO : Epoch 53\n",
      "2018-04-25 14:44:30,727 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:31,741 : INFO : PROGRESS: at 38.55% examples, 89453 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:32,908 : INFO : PROGRESS: at 79.75% examples, 85750 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:32,953 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:32,966 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:32,983 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:32,992 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:33,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:33,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:33,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:33,067 : INFO : training on 363150 raw words (232904 effective words) took 2.3s, 100105 effective words/s\n",
      "2018-04-25 14:44:33,074 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:33,077 : INFO : Epoch 54\n",
      "2018-04-25 14:44:33,089 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:34,452 : INFO : PROGRESS: at 41.17% examples, 71570 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:35,281 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:35,308 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:35,334 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:35,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:35,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:35,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:35,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:35,411 : INFO : training on 363150 raw words (232986 effective words) took 2.3s, 101256 effective words/s\n",
      "2018-04-25 14:44:35,414 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:35,417 : INFO : Epoch 55\n",
      "2018-04-25 14:44:35,429 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:36,463 : INFO : PROGRESS: at 35.60% examples, 82070 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:37,727 : INFO : PROGRESS: at 79.69% examples, 81533 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:37,745 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:37,752 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:37,755 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:37,802 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:37,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:37,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:37,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:37,876 : INFO : training on 363150 raw words (232969 effective words) took 2.4s, 95928 effective words/s\n",
      "2018-04-25 14:44:37,879 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:37,882 : INFO : Epoch 56\n",
      "2018-04-25 14:44:37,899 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:38,960 : INFO : PROGRESS: at 22.11% examples, 49287 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:40,234 : INFO : PROGRESS: at 79.78% examples, 80298 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:40,312 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:40,321 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:40,326 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:40,338 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:40,345 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:40,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:40,394 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:40,399 : INFO : training on 363150 raw words (233047 effective words) took 2.5s, 93989 effective words/s\n",
      "2018-04-25 14:44:40,402 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:40,405 : INFO : Epoch 57\n",
      "2018-04-25 14:44:40,420 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:41,741 : INFO : PROGRESS: at 41.17% examples, 73799 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:42,600 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:42,611 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:42,617 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:42,661 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:42,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:42,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:42,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:42,719 : INFO : training on 363150 raw words (233402 effective words) took 2.3s, 102266 effective words/s\n",
      "2018-04-25 14:44:42,722 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:42,726 : INFO : Epoch 58\n",
      "2018-04-25 14:44:42,738 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:43,752 : INFO : PROGRESS: at 38.45% examples, 89631 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:44,969 : INFO : PROGRESS: at 79.77% examples, 83843 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:44,995 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:45,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:45,007 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:45,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:45,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:45,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:45,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:45,106 : INFO : training on 363150 raw words (233182 effective words) took 2.4s, 99005 effective words/s\n",
      "2018-04-25 14:44:45,110 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:45,114 : INFO : Epoch 59\n",
      "2018-04-25 14:44:45,128 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:46,146 : INFO : PROGRESS: at 32.95% examples, 76316 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:47,248 : INFO : PROGRESS: at 79.77% examples, 88168 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:47,376 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:47,423 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:47,445 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:47,464 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:47,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:47,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:47,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:47,502 : INFO : training on 363150 raw words (232966 effective words) took 2.4s, 98655 effective words/s\n",
      "2018-04-25 14:44:47,504 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:47,508 : INFO : Epoch 60\n",
      "2018-04-25 14:44:47,521 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:48,858 : INFO : PROGRESS: at 41.22% examples, 73113 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:49,733 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:49,746 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:49,752 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:49,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:49,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:49,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:49,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:49,834 : INFO : training on 363150 raw words (233307 effective words) took 2.3s, 101731 effective words/s\n",
      "2018-04-25 14:44:49,837 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:49,840 : INFO : Epoch 61\n",
      "2018-04-25 14:44:49,852 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:50,873 : INFO : PROGRESS: at 38.40% examples, 89601 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:52,006 : INFO : PROGRESS: at 79.42% examples, 86979 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:52,036 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:52,043 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:52,050 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:52,060 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:52,103 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:52,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:52,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:52,156 : INFO : training on 363150 raw words (233087 effective words) took 2.3s, 101972 effective words/s\n",
      "2018-04-25 14:44:52,159 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:52,161 : INFO : Epoch 62\n",
      "2018-04-25 14:44:52,171 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:53,557 : INFO : PROGRESS: at 41.07% examples, 70210 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:54,415 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:54,421 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:54,426 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:54,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:54,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:54,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:54,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:54,510 : INFO : training on 363150 raw words (233332 effective words) took 2.3s, 100472 effective words/s\n",
      "2018-04-25 14:44:54,514 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:54,518 : INFO : Epoch 63\n",
      "2018-04-25 14:44:54,534 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:55,876 : INFO : PROGRESS: at 41.18% examples, 72625 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:56,746 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:56,763 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:56,776 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:56,784 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:56,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:56,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:56,842 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:56,848 : INFO : training on 363150 raw words (233008 effective words) took 2.3s, 101537 effective words/s\n",
      "2018-04-25 14:44:56,852 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:56,855 : INFO : Epoch 64\n",
      "2018-04-25 14:44:56,869 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:58,220 : INFO : PROGRESS: at 41.24% examples, 71971 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:59,058 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:59,063 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:59,069 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:59,072 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:59,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:59,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:59,181 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:59,188 : INFO : training on 363150 raw words (232816 effective words) took 2.3s, 101136 effective words/s\n",
      "2018-04-25 14:44:59,192 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:59,196 : INFO : Epoch 65\n",
      "2018-04-25 14:44:59,209 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:00,236 : INFO : PROGRESS: at 30.13% examples, 70016 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:01,441 : INFO : PROGRESS: at 79.79% examples, 84138 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:01,516 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:01,530 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:01,540 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:01,549 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:01,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:01,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:01,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:01,616 : INFO : training on 363150 raw words (233168 effective words) took 2.4s, 97767 effective words/s\n",
      "2018-04-25 14:45:01,619 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:01,622 : INFO : Epoch 66\n",
      "2018-04-25 14:45:01,635 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:02,648 : INFO : PROGRESS: at 32.94% examples, 76987 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:03,781 : INFO : PROGRESS: at 79.80% examples, 87260 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:03,853 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:03,904 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:03,911 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:03,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:03,942 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:03,952 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:03,985 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:03,990 : INFO : training on 363150 raw words (233414 effective words) took 2.3s, 99711 effective words/s\n",
      "2018-04-25 14:45:03,993 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:03,996 : INFO : Epoch 67\n",
      "2018-04-25 14:45:04,007 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:05,033 : INFO : PROGRESS: at 38.46% examples, 88293 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:06,262 : INFO : PROGRESS: at 79.60% examples, 82780 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:06,275 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:06,284 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:06,295 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:06,305 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:06,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:06,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:06,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:06,386 : INFO : training on 363150 raw words (232974 effective words) took 2.4s, 98397 effective words/s\n",
      "2018-04-25 14:45:06,388 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:06,391 : INFO : Epoch 68\n",
      "2018-04-25 14:45:06,403 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:07,705 : INFO : PROGRESS: at 41.20% examples, 74458 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:08,654 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:08,659 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:08,666 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:08,673 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:08,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:08,695 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:08,724 : INFO : PROGRESS: at 100.00% examples, 100916 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:45:08,729 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:08,731 : INFO : training on 363150 raw words (232961 effective words) took 2.3s, 100589 effective words/s\n",
      "2018-04-25 14:45:08,734 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:08,740 : INFO : Epoch 69\n",
      "2018-04-25 14:45:08,754 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:09,770 : INFO : PROGRESS: at 32.79% examples, 76714 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:10,922 : INFO : PROGRESS: at 79.74% examples, 86269 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:10,960 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:11,034 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:11,040 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:11,050 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:11,061 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:11,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:11,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:11,107 : INFO : training on 363150 raw words (233169 effective words) took 2.3s, 99555 effective words/s\n",
      "2018-04-25 14:45:11,112 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:11,116 : INFO : Epoch 70\n",
      "2018-04-25 14:45:11,129 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:12,500 : INFO : PROGRESS: at 41.19% examples, 71051 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:13,369 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:13,378 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:13,398 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:13,404 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:13,414 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:13,439 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:13,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:13,473 : INFO : training on 363150 raw words (233266 effective words) took 2.3s, 100261 effective words/s\n",
      "2018-04-25 14:45:13,476 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:13,478 : INFO : Epoch 71\n",
      "2018-04-25 14:45:13,493 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:14,890 : INFO : PROGRESS: at 41.11% examples, 69742 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:15,728 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:15,742 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:15,747 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:15,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:15,762 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:15,788 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:15,833 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:15,836 : INFO : training on 363150 raw words (233156 effective words) took 2.3s, 100296 effective words/s\n",
      "2018-04-25 14:45:15,841 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:15,845 : INFO : Epoch 72\n",
      "2018-04-25 14:45:15,860 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:17,221 : INFO : PROGRESS: at 41.11% examples, 71179 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:18,080 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:18,088 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:18,099 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:18,111 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:18,120 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:18,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:18,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:18,193 : INFO : training on 363150 raw words (233128 effective words) took 2.3s, 100432 effective words/s\n",
      "2018-04-25 14:45:18,195 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:18,201 : INFO : Epoch 73\n",
      "2018-04-25 14:45:18,213 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:19,562 : INFO : PROGRESS: at 41.20% examples, 71913 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:20,436 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:20,443 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:20,446 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:20,462 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:20,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:20,496 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:20,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:20,533 : INFO : training on 363150 raw words (232788 effective words) took 2.3s, 100848 effective words/s\n",
      "2018-04-25 14:45:20,536 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:20,538 : INFO : Epoch 74\n",
      "2018-04-25 14:45:20,552 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:21,932 : INFO : PROGRESS: at 41.28% examples, 70402 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:22,810 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:22,820 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:22,831 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:22,840 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:22,847 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:22,878 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:22,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:22,923 : INFO : training on 363150 raw words (233133 effective words) took 2.4s, 98864 effective words/s\n",
      "2018-04-25 14:45:22,927 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:22,931 : INFO : Epoch 75\n",
      "2018-04-25 14:45:22,946 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:24,358 : INFO : PROGRESS: at 41.19% examples, 68792 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:25,207 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:25,213 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:25,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:25,271 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:25,283 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:25,297 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:25,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:25,333 : INFO : training on 363150 raw words (233277 effective words) took 2.4s, 98318 effective words/s\n",
      "2018-04-25 14:45:25,336 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:25,339 : INFO : Epoch 76\n",
      "2018-04-25 14:45:25,354 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:26,753 : INFO : PROGRESS: at 41.30% examples, 70278 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:27,616 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:27,630 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:27,640 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:27,646 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:27,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:27,686 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:27,724 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:27,729 : INFO : training on 363150 raw words (233435 effective words) took 2.3s, 99413 effective words/s\n",
      "2018-04-25 14:45:27,732 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:27,735 : INFO : Epoch 77\n",
      "2018-04-25 14:45:27,746 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:28,779 : INFO : PROGRESS: at 35.78% examples, 82384 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:29,990 : INFO : PROGRESS: at 79.77% examples, 83482 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:30,073 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:30,089 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:30,105 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:30,112 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:30,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:30,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:30,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:30,179 : INFO : training on 363150 raw words (232843 effective words) took 2.4s, 96432 effective words/s\n",
      "2018-04-25 14:45:30,182 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:30,185 : INFO : Epoch 78\n",
      "2018-04-25 14:45:30,195 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:31,210 : INFO : PROGRESS: at 29.92% examples, 70349 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:32,478 : INFO : PROGRESS: at 79.81% examples, 81961 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:32,506 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:32,514 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:32,568 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:32,574 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:32,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:32,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:32,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:32,634 : INFO : training on 363150 raw words (233299 effective words) took 2.4s, 96138 effective words/s\n",
      "2018-04-25 14:45:32,638 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:32,642 : INFO : Epoch 79\n",
      "2018-04-25 14:45:32,655 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:33,677 : INFO : PROGRESS: at 29.88% examples, 69586 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:34,872 : INFO : PROGRESS: at 79.78% examples, 84328 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:34,888 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:34,915 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:34,960 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:34,976 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:34,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:34,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:35,041 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:35,044 : INFO : training on 363150 raw words (233180 effective words) took 2.4s, 98082 effective words/s\n",
      "2018-04-25 14:45:35,048 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:35,050 : INFO : Epoch 80\n",
      "2018-04-25 14:45:35,061 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:36,175 : INFO : PROGRESS: at 21.93% examples, 46720 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:37,506 : INFO : PROGRESS: at 79.79% examples, 76389 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:37,548 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:37,554 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:37,557 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:37,568 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:37,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:37,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:37,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:37,642 : INFO : training on 363150 raw words (232809 effective words) took 2.6s, 90683 effective words/s\n",
      "2018-04-25 14:45:37,644 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:37,647 : INFO : Epoch 81\n",
      "2018-04-25 14:45:37,660 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:39,059 : INFO : PROGRESS: at 41.33% examples, 69616 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:39,895 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:39,909 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:39,917 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:39,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:39,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:39,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:39,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:39,997 : INFO : training on 363150 raw words (232979 effective words) took 2.3s, 100468 effective words/s\n",
      "2018-04-25 14:45:39,999 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:40,002 : INFO : Epoch 82\n",
      "2018-04-25 14:45:40,014 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:41,032 : INFO : PROGRESS: at 30.10% examples, 70017 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:42,221 : INFO : PROGRESS: at 79.79% examples, 84708 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:42,266 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:42,305 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:42,310 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:42,330 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:42,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:42,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:42,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:42,392 : INFO : training on 363150 raw words (233009 effective words) took 2.4s, 98532 effective words/s\n",
      "2018-04-25 14:45:42,395 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:42,399 : INFO : Epoch 83\n",
      "2018-04-25 14:45:42,413 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:43,435 : INFO : PROGRESS: at 24.67% examples, 57830 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:44,644 : INFO : PROGRESS: at 79.75% examples, 84147 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:44,681 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:44,710 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:44,746 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:44,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:44,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:44,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:44,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:44,825 : INFO : training on 363150 raw words (233199 effective words) took 2.4s, 97498 effective words/s\n",
      "2018-04-25 14:45:44,828 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:44,830 : INFO : Epoch 84\n",
      "2018-04-25 14:45:44,848 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:46,260 : INFO : PROGRESS: at 41.17% examples, 68899 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:47,108 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:47,115 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:47,120 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:47,126 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:47,134 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:47,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:47,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:47,214 : INFO : training on 363150 raw words (233202 effective words) took 2.3s, 99367 effective words/s\n",
      "2018-04-25 14:45:47,218 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:47,221 : INFO : Epoch 85\n",
      "2018-04-25 14:45:47,234 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:48,252 : INFO : PROGRESS: at 35.64% examples, 82963 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:49,451 : INFO : PROGRESS: at 79.59% examples, 84424 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:49,476 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:49,481 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:49,485 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:49,507 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:49,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:49,543 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:49,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:49,584 : INFO : training on 363150 raw words (233316 effective words) took 2.3s, 99807 effective words/s\n",
      "2018-04-25 14:45:49,587 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:49,589 : INFO : Epoch 86\n",
      "2018-04-25 14:45:49,601 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:50,672 : INFO : PROGRESS: at 22.01% examples, 48482 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:51,738 : INFO : PROGRESS: at 60.51% examples, 66316 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:52,322 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:52,347 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:52,381 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:52,390 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:52,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:52,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:52,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:52,452 : INFO : training on 363150 raw words (232907 effective words) took 2.8s, 82068 effective words/s\n",
      "2018-04-25 14:45:52,455 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:52,458 : INFO : Epoch 87\n",
      "2018-04-25 14:45:52,468 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:53,482 : INFO : PROGRESS: at 30.14% examples, 70423 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:54,706 : INFO : PROGRESS: at 79.50% examples, 83536 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:54,781 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:54,787 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:54,791 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:54,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:54,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:54,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:54,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:54,870 : INFO : training on 363150 raw words (233140 effective words) took 2.4s, 97555 effective words/s\n",
      "2018-04-25 14:45:54,874 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:54,877 : INFO : Epoch 88\n",
      "2018-04-25 14:45:54,891 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:55,917 : INFO : PROGRESS: at 27.42% examples, 63214 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:57,171 : INFO : PROGRESS: at 79.78% examples, 82098 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:57,212 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:57,226 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:57,230 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:57,236 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:57,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:57,269 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:57,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:57,311 : INFO : training on 363150 raw words (233276 effective words) took 2.4s, 96879 effective words/s\n",
      "2018-04-25 14:45:57,315 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:57,319 : INFO : Epoch 89\n",
      "2018-04-25 14:45:57,335 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:58,373 : INFO : PROGRESS: at 38.55% examples, 87719 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:59,581 : INFO : PROGRESS: at 79.77% examples, 83260 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:59,640 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:59,646 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:59,654 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:59,682 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:59,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:59,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:59,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:59,741 : INFO : training on 363150 raw words (232894 effective words) took 2.4s, 97455 effective words/s\n",
      "2018-04-25 14:45:59,744 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:59,746 : INFO : Epoch 90\n",
      "2018-04-25 14:45:59,762 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:00,787 : INFO : PROGRESS: at 30.11% examples, 69968 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:02,086 : INFO : PROGRESS: at 79.66% examples, 80570 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:02,113 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:02,129 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:02,135 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:02,140 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:02,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:02,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:02,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:02,228 : INFO : training on 363150 raw words (232959 effective words) took 2.4s, 95171 effective words/s\n",
      "2018-04-25 14:46:02,230 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:02,235 : INFO : Epoch 91\n",
      "2018-04-25 14:46:02,246 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:03,434 : INFO : PROGRESS: at 21.88% examples, 43627 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:04,493 : INFO : PROGRESS: at 60.25% examples, 63081 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:46:04,971 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:04,976 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:04,984 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:04,995 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:05,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:05,030 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:05,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:05,079 : INFO : training on 363150 raw words (233060 effective words) took 2.8s, 82637 effective words/s\n",
      "2018-04-25 14:46:05,082 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:05,086 : INFO : Epoch 92\n",
      "2018-04-25 14:46:05,097 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:06,483 : INFO : PROGRESS: at 41.04% examples, 70201 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:07,336 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:07,350 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:07,359 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:07,366 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:07,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:07,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:07,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:07,441 : INFO : training on 363150 raw words (232940 effective words) took 2.3s, 100180 effective words/s\n",
      "2018-04-25 14:46:07,444 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:07,446 : INFO : Epoch 93\n",
      "2018-04-25 14:46:07,457 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:08,481 : INFO : PROGRESS: at 35.67% examples, 82784 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:09,633 : INFO : PROGRESS: at 79.74% examples, 86056 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:09,695 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:09,738 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:09,754 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:09,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:09,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:09,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:09,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:09,819 : INFO : training on 363150 raw words (232883 effective words) took 2.3s, 99305 effective words/s\n",
      "2018-04-25 14:46:09,822 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:09,827 : INFO : Epoch 94\n",
      "2018-04-25 14:46:09,838 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:10,863 : INFO : PROGRESS: at 38.41% examples, 88834 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:12,095 : INFO : PROGRESS: at 79.71% examples, 82859 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:12,119 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:12,128 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:12,136 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:12,142 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:12,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:12,179 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:12,219 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:12,227 : INFO : training on 363150 raw words (232917 effective words) took 2.4s, 98141 effective words/s\n",
      "2018-04-25 14:46:12,229 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:12,233 : INFO : Epoch 95\n",
      "2018-04-25 14:46:12,244 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:13,263 : INFO : PROGRESS: at 35.48% examples, 82761 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:14,472 : INFO : PROGRESS: at 79.33% examples, 83933 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:14,504 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:14,510 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:14,516 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:14,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:14,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:14,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:14,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:14,628 : INFO : training on 363150 raw words (233145 effective words) took 2.4s, 98351 effective words/s\n",
      "2018-04-25 14:46:14,631 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:14,634 : INFO : Epoch 96\n",
      "2018-04-25 14:46:14,650 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:15,666 : INFO : PROGRESS: at 38.56% examples, 89649 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:16,818 : INFO : PROGRESS: at 79.72% examples, 86349 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:16,907 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:16,927 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:16,943 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:16,951 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:16,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:16,985 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:17,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:17,025 : INFO : training on 363150 raw words (233109 effective words) took 2.4s, 98766 effective words/s\n",
      "2018-04-25 14:46:17,028 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:17,032 : INFO : Epoch 97\n",
      "2018-04-25 14:46:17,047 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:18,069 : INFO : PROGRESS: at 27.49% examples, 63808 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:46:19,341 : INFO : PROGRESS: at 79.77% examples, 81617 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:19,353 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:19,365 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:19,380 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:19,392 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:19,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:19,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:19,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:19,465 : INFO : training on 363150 raw words (233272 effective words) took 2.4s, 97095 effective words/s\n",
      "2018-04-25 14:46:19,468 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:19,470 : INFO : Epoch 98\n",
      "2018-04-25 14:46:19,480 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:20,921 : INFO : PROGRESS: at 41.23% examples, 67646 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:21,755 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:21,761 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:21,764 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:21,772 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:21,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:21,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:21,854 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:21,860 : INFO : training on 363150 raw words (233257 effective words) took 2.4s, 98762 effective words/s\n",
      "2018-04-25 14:46:21,863 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:21,866 : INFO : Epoch 99\n",
      "2018-04-25 14:46:21,876 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:23,259 : INFO : PROGRESS: at 40.97% examples, 70455 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:24,168 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:24,175 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:24,183 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:24,192 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:24,209 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:24,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:24,291 : INFO : PROGRESS: at 100.00% examples, 97503 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:46:24,304 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:24,306 : INFO : training on 363150 raw words (233235 effective words) took 2.4s, 96885 effective words/s\n",
      "2018-04-25 14:46:24,310 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:24,315 : INFO : saving Doc2Vec object under ./imdb.d2v, separately None\n",
      "2018-04-25 14:46:24,321 : INFO : not storing attribute syn0norm\n",
      "2018-04-25 14:46:24,326 : INFO : not storing attribute cum_table\n",
      "2018-04-25 14:46:24,763 : INFO : saved ./imdb.d2v\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# shuffle\n",
    "from random import shuffle\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "#import cPickle as pickle   #Note: in python3, _pickle was used instead of cpickle\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "\n",
    "sources = {'./Dataset Processed/txt output_final/n/sum/n_all_test.txt':'TEST_NEG', './Dataset Processed/txt output_final/y/sum/y_all_test.txt':'TEST_POS', './Dataset Processed/txt output_final/n/sum/n_all_train.txt':'TRAIN_NEG', './Dataset Processed/txt output_final/y/sum/y_all_train.txt':'TRAIN_POS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "for epoch in range(100):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(sentences.sentences_perm(),\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter,\n",
    "    )\n",
    "\n",
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:48:12,542 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('orleans.', 0.6345005035400391),\n",
       " ('glow', 0.6184808015823364),\n",
       " ('sticks', 0.5736362934112549),\n",
       " ('funk...i', 0.5610934495925903),\n",
       " ('cod4mw2', 0.5446567535400391),\n",
       " ('frog.', 0.5442479848861694),\n",
       " ('soo', 0.5269519090652466),\n",
       " ('convos', 0.51676344871521),\n",
       " ('buddies...', 0.4994044899940491),\n",
       " ('simplest', 0.4900153875350952)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('wow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x29138053710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loser!!!!', 0.6319977045059204),\n",
       " ('booga', 0.6215949058532715),\n",
       " ('courage!', 0.6092036962509155),\n",
       " ('booga!', 0.6054763793945312),\n",
       " ('me!!!', 0.6025193929672241),\n",
       " ('burg...someone', 0.5988327264785767),\n",
       " ('stranded', 0.5970384478569031),\n",
       " ('Mulan...', 0.5725835561752319),\n",
       " ('???', 0.5678666234016418),\n",
       " ('boom', 0.5648619532585144)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numbers!', 0.783098578453064),\n",
       " ('needz', 0.7755323648452759),\n",
       " ('phone,', 0.7491334676742554),\n",
       " ('number?', 0.7489640712738037),\n",
       " ('fo', 0.7310982942581177),\n",
       " ('what?', 0.7217720746994019),\n",
       " ('birfday,', 0.7177667021751404),\n",
       " ('gotz', 0.6787748336791992),\n",
       " ('diamonds', 0.675055742263794),\n",
       " ('divide//', 0.6602415442466736)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('yo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('haha..', 0.61693274974823),\n",
       " ('compell', 0.6105270385742188),\n",
       " ('scrabble', 0.5972521901130676),\n",
       " ('pack...we', 0.594215989112854),\n",
       " ('Yay...', 0.5926961898803711),\n",
       " ('Jesus:', 0.5818872451782227),\n",
       " ('Day.', 0.578163206577301),\n",
       " ('Columbus', 0.5687612295150757),\n",
       " ('gresy', 0.5616876482963562),\n",
       " ('boyfriend', 0.5551649332046509)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Independence.', 0.6609710454940796),\n",
       " ('Humboldt', 0.6597853899002075),\n",
       " ('declaration', 0.6468594670295715),\n",
       " ('wow...', 0.6215463876724243),\n",
       " ('stories...', 0.6150673031806946),\n",
       " ('Massachusetts.', 0.6051443219184875),\n",
       " ('involving', 0.6025294661521912),\n",
       " ('TJ', 0.5930542945861816),\n",
       " ('aching,', 0.5917971134185791),\n",
       " ('ease', 0.5917605757713318)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TRAGEDY!!!!!!!!!!!', 0.7178655862808228),\n",
       " ('car....yo!', 0.7166972160339355),\n",
       " ('claimed', 0.7024919986724854),\n",
       " ('frnt', 0.6944730281829834),\n",
       " ('evry1!!!!', 0.6852693557739258),\n",
       " ('rockin', 0.6767828464508057),\n",
       " ('statistics.', 0.6531293392181396),\n",
       " ('style', 0.6427193284034729),\n",
       " ('perform', 0.6240640878677368),\n",
       " ('door..mwawaahh', 0.6224106550216675)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((3578,100))\n",
    "train_labels = numpy.zeros((3578))\n",
    "for i in range(1627):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_labels[i] = 1\n",
    "for i in range(1949):\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i+1627] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[i+1627] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3739661e-01, -2.1580108e-01, -4.8335126e-01,  1.1482675e+00,\n",
       "       -4.7941580e-02, -4.4821692e-01,  3.9039350e-01, -1.2100271e+00,\n",
       "       -2.7058744e-01,  4.6899474e-01, -7.1310796e-02, -7.3290646e-01,\n",
       "        3.4350872e-01, -9.1766286e-01, -8.0835754e-01, -6.1130118e-02,\n",
       "        6.5094030e-01,  1.5644893e-01,  9.8188169e-02,  2.9968953e-01,\n",
       "       -9.5501977e-01, -4.3682599e-01, -5.8097118e-01, -6.4757466e-01,\n",
       "       -4.0122551e-01,  3.5669097e-01,  3.6208892e-01,  2.3384538e-01,\n",
       "       -3.6113268e-01,  1.4055296e-03,  2.6255829e-02,  2.3620425e-02,\n",
       "       -3.5107261e-01, -1.4189466e+00,  3.8090906e-01, -5.7261753e-01,\n",
       "        7.8484339e-01, -6.0194635e-01,  7.6612008e-01, -4.1130096e-01,\n",
       "       -5.9421396e-01, -4.5939643e-02,  6.4430970e-01,  6.5054297e-01,\n",
       "       -3.9242661e-01, -1.8527058e-01,  9.4514400e-01,  4.1067389e-01,\n",
       "       -2.3936620e-01,  1.1376140e+00,  6.2980860e-01,  6.9525754e-01,\n",
       "       -4.5010954e-01,  1.2126705e-01, -6.7568219e-01, -3.6877517e-02,\n",
       "        7.5955963e-01, -3.3801693e-01, -7.0519507e-01, -4.9974373e-01,\n",
       "        1.4324425e-01,  3.2810995e-01,  9.3200046e-01,  1.5225296e-01,\n",
       "       -1.3543957e-01, -7.6429829e-02, -2.3031063e-01, -3.7903610e-01,\n",
       "       -5.5908805e-01, -8.4772456e-01, -4.9993908e-01,  8.3023953e-01,\n",
       "       -6.8701231e-01, -1.1085374e-02, -6.1952090e-01, -1.0657929e+00,\n",
       "        7.9374272e-01,  4.9138966e-01,  3.9554825e-01, -1.7643749e+00,\n",
       "       -3.7373623e-01,  8.6625886e-01, -4.9034629e-02, -9.5129527e-02,\n",
       "        3.3110461e-01,  1.0784267e+00,  1.1681069e+00, -1.0257393e+00,\n",
       "       -7.1068859e-01,  7.4806117e-02, -8.1650037e-01,  4.4953513e-01,\n",
       "        2.7567148e-01,  3.2543078e-01, -1.7744846e+00, -1.4016666e-01,\n",
       "        6.2570167e-01,  1.0729808e+00,  3.3692315e-01,  4.2961814e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_NEG_1949']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[1628])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80370075  0.38103873 -0.41746134  0.28001085  0.12257605 -0.27522427\n",
      " -0.0759928   0.44818515  0.1857688  -0.58987349  0.98279178 -0.37254193\n",
      "  0.69479209 -0.15024538 -0.49857754  0.13502854  0.16639599 -0.14383537\n",
      "  0.21795294 -0.09338123 -0.62064594 -0.35703933  0.41458052 -0.79077935\n",
      " -0.57588673 -0.33772352 -1.13289046  0.28825644 -1.06246459 -0.04594202\n",
      "  0.85050446  0.65740472  0.04554173 -0.65110093  0.01392455 -0.00168079\n",
      "  0.01303594 -0.55539489  0.28322324 -0.03220263 -0.17849761 -0.55766553\n",
      "  0.67388767 -1.28418255 -0.62294739  0.09353312 -0.70558155 -0.82815075\n",
      " -0.74368787  0.45761847  0.19435069  0.36589673 -1.32936645 -0.04594028\n",
      " -0.15314017 -0.48406509 -0.2448221   0.0884188   0.11982979 -0.07147586\n",
      " -0.48553157  1.28627694  0.18354133  0.16572593  0.71938622  0.07535198\n",
      " -0.03131915 -1.17586052 -0.45244339 -0.53905445 -0.6917423  -0.23952721\n",
      "  0.70727831 -0.68113184 -0.76739806 -0.61074823  0.9355523  -0.76929212\n",
      "  0.29574177 -0.11356615 -0.2456612  -0.02541027 -0.9174915  -0.19077982\n",
      "  0.50409907  0.15028279  0.81076699 -0.38397548 -0.09198571  0.7155205\n",
      " -0.51208287 -0.68841803  0.8181867  -0.24773917 -0.87482268  0.13176565\n",
      " -0.12423912  0.80236799  0.29084954  0.40151706]\n"
     ]
    }
   ],
   "source": [
    "print(train_arrays[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((1790,100))\n",
    "test_labels = numpy.zeros((1790))\n",
    "for i in range(814):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_labels[i] = 1\n",
    "for i in range(974):\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i+814] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[i+814] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[815])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM setup\n",
    "max_features = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "maxlen = 120\n",
    "batch_size = 64\n",
    "nb_classes = 2   # remaining question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (3578, 100)\n",
      "test shape: (1790, 100)\n"
     ]
    }
   ],
   "source": [
    "print('train shape:', train_arrays.shape)\n",
    "print('test shape:', test_arrays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label shape: (3578,)\n",
      "test label shape: (1790,)\n"
     ]
    }
   ],
   "source": [
    "print('train label shape:', train_labels.shape)\n",
    "print('test label shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(train_labels, nb_classes)\n",
    "Y_test = np_utils.to_categorical(test_labels, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4495672 , -0.25423512, -0.56111175, ...,  0.29188001,\n",
       "        -0.06568915,  0.37901598],\n",
       "       [-0.56589675,  0.6087445 , -0.00931993, ...,  0.32722384,\n",
       "         0.59142751,  0.05790269],\n",
       "       [-0.55978441,  0.29878759, -0.20974684, ...,  0.07978857,\n",
       "         0.49747965,  0.24056254],\n",
       "       ...,\n",
       "       [-0.29563537,  0.79836869, -0.34482062, ...,  0.28694791,\n",
       "         0.2336712 ,  0.08923047],\n",
       "       [-0.61417621,  0.45285723, -0.38387924, ...,  0.17011005,\n",
       "         0.41078898, -0.38659084],\n",
       "       [ 0.20200892,  0.3076812 ,  0.72610861, ..., -0.64545828,\n",
       "        -0.08853067, -0.0299416 ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:15: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-9c30d9fad4f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#                     dropout=0.2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;31m# create an input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch_input_shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                     raise ValueError('The first layer in a '\n\u001b[0m\u001b[0;32m    433\u001b[0m                                      \u001b[1;34m'Sequential model must '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                                      \u001b[1;34m'get an `input_shape` or '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "\n",
    "print('Build LSTM model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(100, 128,dropout=0.2))\n",
    "# model.add(Embedding(\n",
    "# #                     output_dim=EMBEDDING_DIM,\n",
    "# #                     input_dim=100,\n",
    "#                     input_length = maxlen,\n",
    "#                     dropout=0.2))\n",
    "\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_arrays, Y_train, batch_size=batch_size, nb_epoch=100,\n",
    "          validation_data=(test_arrays, Y_test))\n",
    "score, acc = model.evaluate(test_arrays,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(test_arrays, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631284916201117"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a classifier\n",
    "# k is chosen to be square root of number of training example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "model = KNeighborsClassifier(n_neighbors=250)\n",
    "model = model.fit(train_arrays, train_labels)\n",
    "model.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363128491620112"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm_vector = SVC()\n",
    "model_svm_vector = model_svm_vector.fit(train_arrays, train_labels)\n",
    "model_svm_vector.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_labels[0]\n",
    "# result_df_svm_vector[0][2]\n",
    "# result_df_svm_vector.loc[0]\n",
    "# output_svm_vector[1]\n",
    "# len(test_labels)\n",
    "len(output_svm_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 665 310 520\n",
      "Precison: 0.4867549668874172\n",
      "Recall: 0.36117936117936117\n",
      "Accuracy: 0.5360536612632756\n",
      "F1 score: 0.41466854724964736\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "import pandas\n",
    "output_svm_vector = model_svm_vector.predict(test_arrays)\n",
    "result_df_svm_vector = pandas.DataFrame(output_svm_vector)\n",
    "# print(test_labels)\n",
    "# print(result_df_svm_vector)\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for i in range(0,1789):\n",
    "    test_cEXT_val_vector = int(test_labels[i])\n",
    "    result_cEXT_val_vector = int(output_svm_vector[i])\n",
    "    if test_cEXT_val_vector == 1:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631284916201117"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a KNN classifier\n",
    "# k is chosen to be square root of number of training example\n",
    "model_knn_vector = KNeighborsClassifier(n_neighbors=250)\n",
    "model_knn_vector = model_knn_vector.fit(train_arrays, train_labels)\n",
    "model_knn_vector.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 900 75 707\n",
      "Precison: 0.5879120879120879\n",
      "Recall: 0.13144963144963145\n",
      "Accuracy: 0.562884292901062\n",
      "F1 score: 0.21485943775100402\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "import pandas\n",
    "output_knn_vector = model_knn_vector.predict(test_arrays)\n",
    "result_df_knn_vector = pandas.DataFrame(output_knn_vector)\n",
    "# print(test_labels)\n",
    "# print(result_df_svm_vector)\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for i in range(0,1789):\n",
    "    test_cEXT_val_vector = int(test_labels[i])\n",
    "    result_cEXT_val_vector = int(output_knn_vector[i])\n",
    "    if test_cEXT_val_vector == 1:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4212290502793296"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_gnb_vector = GaussianNB()\n",
    "model_gnb_vector = model_gnb_vector.fit(train_arrays, train_labels)\n",
    "model_gnb_vector.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 276 699 336\n",
      "Precison: 0.40611724723874254\n",
      "Recall: 0.5872235872235873\n",
      "Accuracy: 0.42146450531022916\n",
      "F1 score: 0.48016072325464587\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "import pandas\n",
    "output_gnb_vector = model_gnb_vector.predict(test_arrays)\n",
    "result_df_gnb_vector = pandas.DataFrame(output_gnb_vector)\n",
    "# print(test_labels)\n",
    "# print(result_df_svm_vector)\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for i in range(0,1789):\n",
    "    test_cEXT_val_vector = int(test_labels[i])\n",
    "    result_cEXT_val_vector = int(output_gnb_vector[i])\n",
    "    if test_cEXT_val_vector == 1:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try CNN\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Convolution1D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-5e2f418cb45d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# we add a Convolution1D, which will learn nb_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# word group filters of size filter_length:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m model.add(Convolution1D(nb_filter=nb_filter,\n\u001b[0m\u001b[0;32m      7\u001b[0m                         \u001b[0mfilter_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mborder_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Convolution1D' is not defined"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using another approach for LSTM\n",
    "max_features = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "status_data = pandas.read_csv(\"./Dataset Processed/mypersonality_final_classifiedByClass_onlyColumn.csv\",encoding='cp1252')\n",
    "# Sort according to EXT, y first, no latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "status_data = status_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is stuck on Band-Aid brand, cuz Band-Aid's stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just going to grab some raw fish................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saw HP6... funny, lots of awesome awkward sile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Need to re-learn my patterns again... awesome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FREE SLURPEE DAY!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>has GOT to stop waking up at 1pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is not feeling exactly top-notch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>will have too many notifications at the next l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>First day of school done! finances are going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is tired and for some reason is looking forwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2 months of hell yet again... FML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>got to play with fire in front of a crowd... Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>is in love with Ren Fest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3 DAYS!!!!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What a great day!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>loves the Pirate English version of Facebook!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>has a choice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2 DAYS!!!!!!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ren Fest with rain - Still good!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kindly thanks everyone who wished him a happy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>finance class (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GETS TO SLEEP TONIGHT!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3 HOURS LEFT!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>As much as I like our family's mechanic, I rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>is dying!!! HEEEEELPPPPP!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Saturday was easily one of the best days of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>feeling slightly better...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I've done it - i procrastinated enough by play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>is potentially going to be pulling an all-nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>))))))))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6980</th>\n",
       "      <td>TGIF Hopefully I can have some fun too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6981</th>\n",
       "      <td>GO CAVS!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982</th>\n",
       "      <td>CAVS ROCK THE HOUSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983</th>\n",
       "      <td>In Canada with fam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6984</th>\n",
       "      <td>Thank you all.  God bless!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6985</th>\n",
       "      <td>Merry X-Mas!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>is moving today. I've had to move 7 times in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>Thanks for all the Birthday and Christmas Wish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>Seriously, if a guy that you're dating ever sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>Still waiting....so glad Monday's a holiday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>Why did they discontinue naptime after kinderg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>Yay, finally got my computer set up at my hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>UGGGGGGHHHHH! My WoW account is totally screwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>Wowsa....greeeeeeeat job news down the grapevi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>I feel like I'm drowning in paper work, uggh! :X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>Brrrrrrrrrrrrrrrr, cold day! Where are those 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>Awesome! First sunny day in ages...but now it'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>Woot woot! Good week...super productive and bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>I hope it snows enough to go sledding...who wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>Crap, I think my eyes hate me. Far away stuff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>Rain, rain, go away, don't come back 'til I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>It's not that I can't live without you; it's t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>It's a beautiful day in a neighborhood, a beau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>Totalled my car last night. Luckilly no one wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>Oh well.  Only two things to do when your team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>And the cabin fever begins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>Facebook me marea. Me hates it long time T-T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>snipers get more head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>Last night was amazing! Not only did I see PRO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7010 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 STATUS\n",
       "0     is stuck on Band-Aid brand, cuz Band-Aid's stu...\n",
       "1      Just going to grab some raw fish................\n",
       "2     saw HP6... funny, lots of awesome awkward sile...\n",
       "3      Need to re-learn my patterns again... awesome...\n",
       "4                                   FREE SLURPEE DAY!!!\n",
       "5                   has GOT to stop waking up at 1pm...\n",
       "6                   is not feeling exactly top-notch...\n",
       "7     will have too many notifications at the next l...\n",
       "8     First day of school done! finances are going t...\n",
       "9     is tired and for some reason is looking forwar...\n",
       "10                    2 months of hell yet again... FML\n",
       "11    got to play with fire in front of a crowd... Y...\n",
       "12                          is in love with Ren Fest...\n",
       "13                                  3 DAYS!!!!!!!!!!!!!\n",
       "14                                  What a great day!!!\n",
       "15      loves the Pirate English version of Facebook!!!\n",
       "16                                      has a choice...\n",
       "17                                2 DAYS!!!!!!!!!!!!!!!\n",
       "18                   Ren Fest with rain - Still good!!!\n",
       "19    kindly thanks everyone who wished him a happy ...\n",
       "20                                      finance class (\n",
       "21                       GETS TO SLEEP TONIGHT!!!!!!!!!\n",
       "22                               3 HOURS LEFT!!!!!!!!!!\n",
       "23    As much as I like our family's mechanic, I rea...\n",
       "24                        is dying!!! HEEEEELPPPPP!!!!!\n",
       "25    Saturday was easily one of the best days of my...\n",
       "26                           feeling slightly better...\n",
       "27    I've done it - i procrastinated enough by play...\n",
       "28    is potentially going to be pulling an all-nigh...\n",
       "29                                             ))))))))\n",
       "...                                                 ...\n",
       "6980          TGIF Hopefully I can have some fun too...\n",
       "6981                                           GO CAVS!\n",
       "6982                                CAVS ROCK THE HOUSE\n",
       "6983                                 In Canada with fam\n",
       "6984                         Thank you all.  God bless!\n",
       "6985                                       Merry X-Mas!\n",
       "6986  is moving today. I've had to move 7 times in t...\n",
       "6987                                              black\n",
       "6988  Thanks for all the Birthday and Christmas Wish...\n",
       "6989  Seriously, if a guy that you're dating ever sa...\n",
       "6990       Still waiting....so glad Monday's a holiday!\n",
       "6991  Why did they discontinue naptime after kinderg...\n",
       "6992  Yay, finally got my computer set up at my hous...\n",
       "6993  UGGGGGGHHHHH! My WoW account is totally screwe...\n",
       "6994  Wowsa....greeeeeeeat job news down the grapevi...\n",
       "6995   I feel like I'm drowning in paper work, uggh! :X\n",
       "6996  Brrrrrrrrrrrrrrrr, cold day! Where are those 4...\n",
       "6997  Awesome! First sunny day in ages...but now it'...\n",
       "6998  Woot woot! Good week...super productive and bu...\n",
       "6999  I hope it snows enough to go sledding...who wa...\n",
       "7000  Crap, I think my eyes hate me. Far away stuff ...\n",
       "7001  Rain, rain, go away, don't come back 'til I'm ...\n",
       "7002  It's not that I can't live without you; it's t...\n",
       "7003  It's a beautiful day in a neighborhood, a beau...\n",
       "7004  Totalled my car last night. Luckilly no one wa...\n",
       "7005  Oh well.  Only two things to do when your team...\n",
       "7006                        And the cabin fever begins.\n",
       "7007       Facebook me marea. Me hates it long time T-T\n",
       "7008                              snipers get more head\n",
       "7009  Last night was amazing! Not only did I see PRO...\n",
       "\n",
       "[7010 rows x 1 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(status_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       is stuck on Band-Aid brand, cuz Band-Aid's stu...\n",
       "1        Just going to grab some raw fish................\n",
       "2       saw HP6... funny, lots of awesome awkward sile...\n",
       "3        Need to re-learn my patterns again... awesome...\n",
       "4                                     FREE SLURPEE DAY!!!\n",
       "5                     has GOT to stop waking up at 1pm...\n",
       "6                     is not feeling exactly top-notch...\n",
       "7       will have too many notifications at the next l...\n",
       "8       First day of school done! finances are going t...\n",
       "9       is tired and for some reason is looking forwar...\n",
       "10                      2 months of hell yet again... FML\n",
       "11      got to play with fire in front of a crowd... Y...\n",
       "12                            is in love with Ren Fest...\n",
       "13                                    3 DAYS!!!!!!!!!!!!!\n",
       "14                                    What a great day!!!\n",
       "15        loves the Pirate English version of Facebook!!!\n",
       "16                                        has a choice...\n",
       "17                                  2 DAYS!!!!!!!!!!!!!!!\n",
       "18                     Ren Fest with rain - Still good!!!\n",
       "19      kindly thanks everyone who wished him a happy ...\n",
       "20                                        finance class (\n",
       "21                         GETS TO SLEEP TONIGHT!!!!!!!!!\n",
       "22                                 3 HOURS LEFT!!!!!!!!!!\n",
       "23      As much as I like our family's mechanic, I rea...\n",
       "24                          is dying!!! HEEEEELPPPPP!!!!!\n",
       "25      Saturday was easily one of the best days of my...\n",
       "26                             feeling slightly better...\n",
       "27      I've done it - i procrastinated enough by play...\n",
       "28      is potentially going to be pulling an all-nigh...\n",
       "29                                               ))))))))\n",
       "                              ...                        \n",
       "4646    can't wait to see The Princess and the Frog to...\n",
       "4647    PROPNAME and PROPNAME are my two middle names....\n",
       "4648    can't wait for class tomorrow. I ? the 1st day...\n",
       "4649                   had cake & ice cream for breakfast\n",
       "4650    M.I.A. until I finish my projects, tests and C...\n",
       "4651    Thanks friends, I had a really special birthda...\n",
       "4652                          misses the beach already...\n",
       "4653    loved The Princess and the Frog...I almost cri...\n",
       "4654    excited...my show is on in 30. RHWNJ reunion, ...\n",
       "4655    do not EVER book your trips through Travelocit...\n",
       "4656    just finished the CSET. didn't think it was th...\n",
       "4657                       I ? my Flip Video cam so much!\n",
       "4658                      just booked VEGAS. So excited!!\n",
       "4659    is really really sore from dance yesterday! I ...\n",
       "4660    made my 1st iMovie! My fave clips of me dancin...\n",
       "4661    I have a love//hate relationship with the Stai...\n",
       "4662    new favorite cocktail = Champagne & Chambord.....\n",
       "4663        Monday a.m. Yoga...sigh my life is SO hard ;)\n",
       "4664    got a new shampoo//conditioner and I'm excited...\n",
       "4665    is thinking of doing BOTH Stairmaster//booty e...\n",
       "4666    Does anyone have a stud finder and a drill I c...\n",
       "4667    is excited for PROPNAME's Birthday! Don't forg...\n",
       "4668                                            leopard ?\n",
       "4669    passed all 3 subtests of the CSET in one go, a...\n",
       "4670    this is the last Sunday I have to work. Yess! ...\n",
       "4671    re-arranged all the furniture in my room today...\n",
       "4672    had a super fun sleepover with PROPNAME and PR...\n",
       "4673    happy I was able to squeeze in a 30 minute pol...\n",
       "4674    is all tricked out...dang that pole class was ...\n",
       "4675    ok I think I worked out too much today...every...\n",
       "Name: STATUS, Length: 4676, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data.ix[0:4675,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into training and test data: 66% and 33%\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_data, test_data = train_test_split(status_data, test_size=0.50)\n",
    "\n",
    "train_data = status_data.ix[0:3500-1,0]\n",
    "test_data = status_data.ix[3500-1:4998,0]\n",
    "\n",
    "train = train_data.values\n",
    "test = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels_new = numpy.zeros((3500))\n",
    "test_labels_new = numpy.zeros((1500))\n",
    "for i in range(3500):\n",
    "    train_labels_new[i] = 1\n",
    "for i in range(1500):\n",
    "    test_labels_new[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       is stuck on Band-Aid brand, cuz Band-Aid's stu...\n",
       "1        Just going to grab some raw fish................\n",
       "2       saw HP6... funny, lots of awesome awkward sile...\n",
       "3        Need to re-learn my patterns again... awesome...\n",
       "4                                     FREE SLURPEE DAY!!!\n",
       "5                     has GOT to stop waking up at 1pm...\n",
       "6                     is not feeling exactly top-notch...\n",
       "7       will have too many notifications at the next l...\n",
       "8       First day of school done! finances are going t...\n",
       "9       is tired and for some reason is looking forwar...\n",
       "10                      2 months of hell yet again... FML\n",
       "11      got to play with fire in front of a crowd... Y...\n",
       "12                            is in love with Ren Fest...\n",
       "13                                    3 DAYS!!!!!!!!!!!!!\n",
       "14                                    What a great day!!!\n",
       "15        loves the Pirate English version of Facebook!!!\n",
       "16                                        has a choice...\n",
       "17                                  2 DAYS!!!!!!!!!!!!!!!\n",
       "18                     Ren Fest with rain - Still good!!!\n",
       "19      kindly thanks everyone who wished him a happy ...\n",
       "20                                        finance class (\n",
       "21                         GETS TO SLEEP TONIGHT!!!!!!!!!\n",
       "22                                 3 HOURS LEFT!!!!!!!!!!\n",
       "23      As much as I like our family's mechanic, I rea...\n",
       "24                          is dying!!! HEEEEELPPPPP!!!!!\n",
       "25      Saturday was easily one of the best days of my...\n",
       "26                             feeling slightly better...\n",
       "27      I've done it - i procrastinated enough by play...\n",
       "28      is potentially going to be pulling an all-nigh...\n",
       "29                                               ))))))))\n",
       "                              ...                        \n",
       "3470    can't sleep because the dog  keeps barking at ...\n",
       "3471    rediscovered an old journal and some old poetr...\n",
       "3472    I find it interesting that I cannot reply to c...\n",
       "3473    we have to continually be jumping off cliffs a...\n",
       "3474                                   which mustache?!?!\n",
       "3475    is missing Isla Vista hardcore tonight :( I ne...\n",
       "3476    is getting everything to work out despite the ...\n",
       "3477    is going to the city tonight even though she's...\n",
       "3478    got burned by steak grease and has blisters on...\n",
       "3479    I think we're walking step in step, But you do...\n",
       "3480    Is packing up old baggage, mailing it away to ...\n",
       "3481    see, when you forgive your imperfections, and ...\n",
       "3482    is getting on her longboard for the first time...\n",
       "3483    is stoked for West Beach today!!! Too bad its ...\n",
       "3484    both of the towns I live in have been on fire ...\n",
       "3485    is regaining her colors and fitting into her s...\n",
       "3486    is repairing her very broken math skills one e...\n",
       "3487    all we have is hope and love, so don't you wor...\n",
       "3488    is headin downtown for some live music on this...\n",
       "3489    is so stoked to see PROPNAME, PROPNAME and PRO...\n",
       "3490    is preparing for five days of awesomeness. Hur...\n",
       "3491    is cuddleing with PROPNAME, the cutest puppy E...\n",
       "3492    is living on cloud nine... aka in the Montecit...\n",
       "3493    has a LONG day in the field tomorrow and then ...\n",
       "3494    LOVES her Field Fridays and BEER HOUR! Thanks ...\n",
       "3495    is spending her day off watching Dexter instea...\n",
       "3496    is thankful for her quite patio to watch the m...\n",
       "3497    all we have is hope and love, so don't you wor...\n",
       "3498                   is anyone home in this ghost town?\n",
       "3499    is about to go for a thankgiving hike with her...\n",
       "Name: STATUS, Length: 3500, dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3499    is about to go for a thankgiving hike with her...\n",
       "3500    is thankful for a warm fire, good wine and her...\n",
       "3501    downtown or international party? hmmm decision...\n",
       "3502    can't believe she's leaving the lodge in the m...\n",
       "3503    how can I be expected to work when there is so...\n",
       "3504    A simple smile can change a day, an understand...\n",
       "3505    had a really stressful day at work but got a 4...\n",
       "3506       just finished her last quarter at UCSB. Crazy.\n",
       "3507    is very happy with her grades and even happier...\n",
       "3508    come with me, come with me, we'll travel to in...\n",
       "3509    folk and tribal arts festival at the museum of...\n",
       "3510    is exhausted but so ready to see her ladies to...\n",
       "3511    just got back from the reel rock film show and...\n",
       "3512    is applying for the Peace Corps//Master's Inte...\n",
       "3513    you don't need no reason nor a three peice sui...\n",
       "3514    is overwhelmed by data... and this is supposed...\n",
       "3515    finally got her computer back in shape! Open o...\n",
       "3516    roast + wine + family + south park = a very ha...\n",
       "3517    will not be going on the fall 2009 semester at...\n",
       "3518            snow snow snow SNOW!!!!!! HERE I COME!!!!\n",
       "3519    got distracted in Borders for over three hours...\n",
       "3520    finished her stats paper and bought her ticket...\n",
       "3521    is waging a vendetta against UCSB parking serv...\n",
       "3522    Does anyone have malarone (anti-malaria med) t...\n",
       "3523                       is overwHelmed with excitement\n",
       "3524    is headin to the doctor yet again but then Reg...\n",
       "3525    is gettin' on the road, feelin' fine. I'll be ...\n",
       "3526    had a great time at the Devil Makes Three new ...\n",
       "3527                                   Is in Hawaii, yay!\n",
       "3528    is in Hawaii yay! No more phone after this... ...\n",
       "                              ...                        \n",
       "4969                    ugh. dont feel good again.....ugh\n",
       "4970        made fresh chocolate chip cookies...yummmmyyy\n",
       "4971                          today feels like forever!!!\n",
       "4972             got my transformers shirt. its cuute =]]\n",
       "4973                   wants the new Tokio Hotel c.d. =]]\n",
       "4974             got my trnasformers shirt. its cuute =]]\n",
       "4975    PROPNAME always takes too long to get here in ...\n",
       "4976    is a happy camper because i got the tokio hote...\n",
       "4977                doing good with soc. final so far =]]\n",
       "4978                   i hate it when my mom leaves...=[[\n",
       "4979                       mom's coming home tonight. =]]\n",
       "4980             finally home from a loong day at school.\n",
       "4981    disappointed in someone who i thought i would ...\n",
       "4982                             olive garden tonight =]]\n",
       "4983                             had funn at 6 flags! =]]\n",
       "4984    so exhausted today, i don't want it to rain. b...\n",
       "4985                    2 pages down on my paper 3 to go!\n",
       "4986                       gotta love writers block...ugh\n",
       "4987    I don't want to go to class today carrying a s...\n",
       "4988                       done with my paper finally =]]\n",
       "4989    if you know someone who has or had cancer! ? A...\n",
       "4990                so far...i have an A in Sociology =]]\n",
       "4991    i had an awesome idea for my paper last night ...\n",
       "4992             I love the smell of Christmas trees. =]]\n",
       "4993    Thank God I don't have to see PROPNAME ever ag...\n",
       "4994                had a lot of funn with the sister =]]\n",
       "4995            created my plan A schedule of classes =]]\n",
       "4996                  taking a break from my looong paper\n",
       "4997          relaxing and listening to music. funn stuff\n",
       "4998    ugh, starting my soc paper all over....this is...\n",
       "Name: STATUS, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "sequences_train = tokenizer.texts_to_sequences(train_data)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train_new shape: (3500, 80)\n",
      "X_test_new shape: (1500, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "from keras.preprocessing import sequence\n",
    "X_train_new = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_new = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_labels_new, nb_classes)\n",
    "Y_test = np_utils.to_categorical(test_labels_new, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train_new shape:', X_train_new.shape)\n",
    "print('X_test_new shape:', X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 56s - loss: 0.0358 - acc: 0.9848 - val_loss: 4.7154 - val_acc: 0.6000\n",
      "1500/1500 [==============================] - 4s     \n",
      "Test score: 4.715363881429036\n",
      "Test accuracy: 0.5999999642372131\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "# Epoch = 1\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_new, Y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test_new, Y_test))\n",
    "score, acc = model.evaluate(X_test_new, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/100\n",
      "3500/3500 [==============================] - 49s - loss: 0.0323 - acc: 0.9865 - val_loss: 4.7983 - val_acc: 0.6000\n",
      "Epoch 2/100\n",
      "3500/3500 [==============================] - 40s - loss: 8.9578e-06 - acc: 1.0000 - val_loss: 5.0715 - val_acc: 0.6000\n",
      "Epoch 3/100\n",
      "3500/3500 [==============================] - 42s - loss: 5.0913e-06 - acc: 1.0000 - val_loss: 5.2681 - val_acc: 0.6000\n",
      "Epoch 4/100\n",
      "3500/3500 [==============================] - 43s - loss: 3.4498e-06 - acc: 1.0000 - val_loss: 5.3904 - val_acc: 0.6000\n",
      "Epoch 5/100\n",
      "3500/3500 [==============================] - 44s - loss: 2.5867e-06 - acc: 1.0000 - val_loss: 5.4898 - val_acc: 0.6000\n",
      "Epoch 6/100\n",
      "3500/3500 [==============================] - 48s - loss: 2.0341e-06 - acc: 1.0000 - val_loss: 5.5658 - val_acc: 0.6000\n",
      "Epoch 7/100\n",
      "3500/3500 [==============================] - 46s - loss: 1.6649e-06 - acc: 1.0000 - val_loss: 5.6317 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.3738e-06 - acc: 1.0000 - val_loss: 5.6927 - val_acc: 0.6000\n",
      "Epoch 9/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.1766e-06 - acc: 1.0000 - val_loss: 5.7418 - val_acc: 0.6000\n",
      "Epoch 10/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0065e-06 - acc: 1.0000 - val_loss: 5.7834 - val_acc: 0.6000\n",
      "Epoch 11/100\n",
      "3500/3500 [==============================] - 41s - loss: 8.9051e-07 - acc: 1.0000 - val_loss: 5.8336 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      "3500/3500 [==============================] - 41s - loss: 7.8654e-07 - acc: 1.0000 - val_loss: 5.8590 - val_acc: 0.6000\n",
      "Epoch 13/100\n",
      "3500/3500 [==============================] - 44s - loss: 7.0864e-07 - acc: 1.0000 - val_loss: 5.8976 - val_acc: 0.6000\n",
      "Epoch 14/100\n",
      "3500/3500 [==============================] - 41s - loss: 6.3070e-07 - acc: 1.0000 - val_loss: 5.9277 - val_acc: 0.6000\n",
      "Epoch 15/100\n",
      "3500/3500 [==============================] - 40s - loss: 5.7251e-07 - acc: 1.0000 - val_loss: 5.9337 - val_acc: 0.6000\n",
      "Epoch 16/100\n",
      "3500/3500 [==============================] - 71s - loss: 5.2062e-07 - acc: 1.0000 - val_loss: 5.9877 - val_acc: 0.6000\n",
      "Epoch 17/100\n",
      "3500/3500 [==============================] - 94s - loss: 4.7708e-07 - acc: 1.0000 - val_loss: 5.9967 - val_acc: 0.6000\n",
      "Epoch 18/100\n",
      "3500/3500 [==============================] - 41s - loss: 4.4209e-07 - acc: 1.0000 - val_loss: 6.0225 - val_acc: 0.6000\n",
      "Epoch 19/100\n",
      "3500/3500 [==============================] - 44s - loss: 4.0586e-07 - acc: 1.0000 - val_loss: 6.0229 - val_acc: 0.6000\n",
      "Epoch 20/100\n",
      "3500/3500 [==============================] - 41s - loss: 3.7956e-07 - acc: 1.0000 - val_loss: 6.0229 - val_acc: 0.6000\n",
      "Epoch 21/100\n",
      "3500/3500 [==============================] - 39s - loss: 3.5390e-07 - acc: 1.0000 - val_loss: 6.0234 - val_acc: 0.6000\n",
      "Epoch 22/100\n",
      "3500/3500 [==============================] - 55s - loss: 3.2874e-07 - acc: 1.0000 - val_loss: 6.0448 - val_acc: 0.6000\n",
      "Epoch 23/100\n",
      "3500/3500 [==============================] - 53s - loss: 3.0947e-07 - acc: 1.0000 - val_loss: 6.0902 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "3500/3500 [==============================] - 39s - loss: 2.9091e-07 - acc: 1.0000 - val_loss: 6.0910 - val_acc: 0.6000\n",
      "Epoch 25/100\n",
      "3500/3500 [==============================] - 39s - loss: 2.7307e-07 - acc: 1.0000 - val_loss: 6.0924 - val_acc: 0.6000\n",
      "Epoch 26/100\n",
      "3500/3500 [==============================] - 38s - loss: 2.5789e-07 - acc: 1.0000 - val_loss: 6.1338 - val_acc: 0.6000\n",
      "Epoch 27/100\n",
      "3500/3500 [==============================] - 37s - loss: 2.4406e-07 - acc: 1.0000 - val_loss: 6.1348 - val_acc: 0.6000\n",
      "Epoch 28/100\n",
      "3500/3500 [==============================] - 37s - loss: 2.3416e-07 - acc: 1.0000 - val_loss: 6.1348 - val_acc: 0.6000\n",
      "Epoch 29/100\n",
      "3500/3500 [==============================] - 37s - loss: 2.2005e-07 - acc: 1.0000 - val_loss: 6.1348 - val_acc: 0.6000\n",
      "Epoch 30/100\n",
      "3500/3500 [==============================] - 39s - loss: 2.1169e-07 - acc: 1.0000 - val_loss: 6.1431 - val_acc: 0.6000\n",
      "Epoch 31/100\n",
      "3500/3500 [==============================] - 38s - loss: 1.9973e-07 - acc: 1.0000 - val_loss: 6.1924 - val_acc: 0.6000\n",
      "Epoch 32/100\n",
      "3500/3500 [==============================] - 36s - loss: 1.9084e-07 - acc: 1.0000 - val_loss: 6.1924 - val_acc: 0.6000\n",
      "Epoch 33/100\n",
      "3500/3500 [==============================] - 39s - loss: 1.8281e-07 - acc: 1.0000 - val_loss: 6.1924 - val_acc: 0.6000\n",
      "Epoch 34/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.7422e-07 - acc: 1.0000 - val_loss: 6.1924 - val_acc: 0.6000\n",
      "Epoch 35/100\n",
      "3500/3500 [==============================] - 36s - loss: 1.6786e-07 - acc: 1.0000 - val_loss: 6.1937 - val_acc: 0.6000\n",
      "Epoch 36/100\n",
      "3500/3500 [==============================] - 36s - loss: 1.6191e-07 - acc: 1.0000 - val_loss: 6.2581 - val_acc: 0.6000\n",
      "Epoch 37/100\n",
      "3500/3500 [==============================] - 36s - loss: 1.5540e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 38/100\n",
      "3500/3500 [==============================] - 36s - loss: 1.5050e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 39/100\n",
      "3500/3500 [==============================] - 35s - loss: 1.4606e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "3500/3500 [==============================] - 36s - loss: 1.4184e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 41/100\n",
      "3500/3500 [==============================] - 37s - loss: 1.3945e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 42/100\n",
      "3500/3500 [==============================] - 37s - loss: 1.3670e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 43/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.3475e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 44/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.3249e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 45/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.2977e-07 - acc: 1.0000 - val_loss: 6.2735 - val_acc: 0.6000\n",
      "Epoch 46/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.2696e-07 - acc: 1.0000 - val_loss: 6.2758 - val_acc: 0.6000\n",
      "Epoch 47/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.2313e-07 - acc: 1.0000 - val_loss: 6.4119 - val_acc: 0.6000\n",
      "Epoch 48/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.1983e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 49/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.1668e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 50/100\n",
      "3500/3500 [==============================] - 44s - loss: 1.1344e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 51/100\n",
      "3500/3500 [==============================] - 44s - loss: 1.1179e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 52/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0981e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 53/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0834e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 54/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0715e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 55/100\n",
      "3500/3500 [==============================] - 46s - loss: 1.0621e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 56/100\n",
      "3500/3500 [==============================] - 44s - loss: 1.0632e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 57/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0564e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 58/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0525e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 59/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0500e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 60/100\n",
      "3500/3500 [==============================] - 44s - loss: 1.0477e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 61/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0448e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 62/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0450e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 63/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0444e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 64/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0447e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 65/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0431e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 66/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0424e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 67/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0416e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 68/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0416e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 69/100\n",
      "3500/3500 [==============================] - 40s - loss: 1.0420e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 70/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0416e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 71/100\n",
      "3500/3500 [==============================] - 45s - loss: 1.0413e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 72/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0408e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 73/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0404e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 74/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0400e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 75/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0400e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 76/100\n",
      "3500/3500 [==============================] - 44s - loss: 1.0404e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 77/100\n",
      "3500/3500 [==============================] - 47s - loss: 1.0400e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 78/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0399e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 79/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0395e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 80/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0398e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 81/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0393e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 82/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0395e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 83/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0394e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 84/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0395e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 85/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0391e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 86/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0387e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 87/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0393e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 88/100\n",
      "3500/3500 [==============================] - 43s - loss: 1.0387e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 89/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0388e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 90/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0388e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 91/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0387e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 92/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0389e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 93/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0389e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 94/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0394e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 95/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0388e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 96/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0387e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 97/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0388e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 98/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0388e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 99/100\n",
      "3500/3500 [==============================] - 41s - loss: 1.0389e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "Epoch 100/100\n",
      "3500/3500 [==============================] - 42s - loss: 1.0388e-07 - acc: 1.0000 - val_loss: 6.4121 - val_acc: 0.6000\n",
      "1500/1500 [==============================] - 4s     \n",
      "Test score: 6.412095069885254\n",
      "Test accuracy: 0.5999999642372131\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "# Epoch = 100\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_new, Y_train, batch_size=batch_size, nb_epoch=100,\n",
    "          validation_data=(X_test_new, Y_test))\n",
    "score, acc = model.evaluate(X_test_new, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 38s - loss: 0.0747 - acc: 0.9690 - val_loss: 4.3185 - val_acc: 0.6000\n",
      "1500/1500 [==============================] - 4s     \n",
      "Test score: 4.318496852874756\n",
      "Test accuracy: 0.5999999642372131\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "# Epoch = 1\n",
    "# Expanding batch size\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_new, Y_train, batch_size=64, nb_epoch=1,\n",
    "          validation_data=(X_test_new, Y_test))\n",
    "score, acc = model.evaluate(X_test_new, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 33s - loss: 0.0711 - acc: 0.9697 - val_loss: 4.7807 - val_acc: 0.6000\n",
      "1500/1500 [==============================] - 3s     \n",
      "Test score: 4.780679239908854\n",
      "Test accuracy: 0.5999999642372131\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "# Epoch = 1\n",
    "# Without dropout\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_new, Y_train, batch_size=64, nb_epoch=1,\n",
    "          validation_data=(X_test_new, Y_test))\n",
    "score, acc = model.evaluate(X_test_new, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 17s - loss: 0.1369 - acc: 0.9427 - val_loss: 3.6020 - val_acc: 0.6000\n",
      "1472/1500 [============================>.] - ETA: 0sTest score: 3.602019158681234\n",
      "Test accuracy: 0.5999999642372131\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "# Epoch = 1\n",
    "# Without dropout\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 64))\n",
    "model.add(LSTM(64)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_new, Y_train, batch_size=64, nb_epoch=1,\n",
    "          validation_data=(X_test_new, Y_test))\n",
    "score, acc = model.evaluate(X_test_new, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
